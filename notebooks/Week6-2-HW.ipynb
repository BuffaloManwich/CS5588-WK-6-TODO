{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2519a2-2ce5-4127-a274-5b1852e7f865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manny-buff/venvs/core-rag/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment probe written to: /home/manny-buff/projects/capstone/week6-rag-graph/configs/env_rag_graph.json\n",
      "{\n",
      "  \"python_venv\": \"/home/manny-buff/venvs/core-rag\",\n",
      "  \"python_version\": \"3.11.9\",\n",
      "  \"platform\": \"Linux-6.14.0-33-generic-x86_64-with-glibc2.39\",\n",
      "  \"cuda_visible_devices\": null,\n",
      "  \"nvidia_smi\": \"NVIDIA GeForce RTX 4080, 580.65.06, 16376 MiB\",\n",
      "  \"which_python\": \"/home/manny-buff/venvs/core-rag/bin/python\",\n",
      "  \"pip_freeze_head\": \"accelerate==1.10.1\\nacres==0.5.0\\naiofiles==24.1.0\\naiohappyeyeballs==2.6.1\\naiohttp==3.12.15\\naiosignal==1.4.0\\naiosqlite==0.21.0\\nannotated-types==0.7.0\\nanyio==4.10.0\\nargon2-cffi==25.1.0\\nargon2-cffi-bindings==25.1.0\\narrow==1.3.0\\nasttokens==3.0.0\\nasync-lru==2.0.5\\nattrs==25.3.0\\nav==15.1.0\\nbabel==2.17.0\\nbackoff==2.2.1\\nbanks==2.2.0\\nbcrypt==4.3.0\",\n",
      "  \"packages\": {\n",
      "    \"numpy\": \"2.2.1\",\n",
      "    \"pandas\": \"2.2.3\",\n",
      "    \"networkx\": \"3.3\",\n",
      "    \"sentence_transformers\": \"3.0.1\",\n",
      "    \"transformers\": \"4.56.2\",\n",
      "    \"accelerate\": \"1.10.1\",\n",
      "    \"faiss\": \"1.10.0\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell A: Environment Probe'\n",
    "Purpose:\n",
    "- Detect Python version, CUDA toolkits (if available), GPU info, and key package versions.\n",
    "- Save a merged record into configs/env_rag_graph.json for reproducibility.\n",
    "\n",
    "Notes:\n",
    "- Uses only standard libs + minimal imports to avoid heavy loads here.\n",
    "\"\"\"\n",
    "\n",
    "import json, os, sys, subprocess, shutil, platform\n",
    "from pathlib import Path\n",
    "\n",
    "# 'Paths and files' - adjust only if your project layout changes\n",
    "ROOT = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph\")\n",
    "CFG  = ROOT / \"configs\" / \"env_rag_graph.json\"\n",
    "\n",
    "def cmd_out(args):\n",
    "    # 'Run a shell command safely and return stdout text'\n",
    "    try:\n",
    "        return subprocess.check_output(args, stderr=subprocess.STDOUT, text=True).strip()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "# 'Collect environment info'\n",
    "info = {\n",
    "    \"python_venv\": str(Path.home() / \"venvs\" / \"core-rag\"),\n",
    "    \"python_version\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cuda_visible_devices\": os.environ.get(\"CUDA_VISIBLE_DEVICES\", None),\n",
    "    \"nvidia_smi\": cmd_out([\"bash\", \"-lc\", \"nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv,noheader\"]),\n",
    "    \"which_python\": cmd_out([\"bash\", \"-lc\", \"which python\"]),\n",
    "    \"pip_freeze_head\": cmd_out([\"bash\", \"-lc\", \"pip freeze | head -n 20\"])\n",
    "}\n",
    "\n",
    "# 'Key packages versions' - quick imports to record versions\n",
    "versions = {}\n",
    "for pkg in [\"numpy\", \"pandas\", \"networkx\", \"sentence_transformers\", \"transformers\", \"accelerate\", \"faiss\"]:\n",
    "    try:\n",
    "        mod = __import__(pkg)\n",
    "        versions[pkg] = getattr(mod, \"__version__\", \"unknown\")\n",
    "    except Exception as e:\n",
    "        versions[pkg] = f\"not importable: {e}\"\n",
    "\n",
    "info[\"packages\"] = versions\n",
    "\n",
    "# 'Merge with existing json'\n",
    "CFG.parent.mkdir(parents=True, exist_ok=True)\n",
    "existing = {}\n",
    "if CFG.exists():\n",
    "    try:\n",
    "        existing = json.loads(CFG.read_text())\n",
    "    except Exception:\n",
    "        existing = {}\n",
    "\n",
    "existing.update(info)\n",
    "CFG.write_text(json.dumps(existing, indent=2))\n",
    "\n",
    "print(\"Environment probe written to:\", CFG)\n",
    "print(json.dumps(info, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "857c4935-ca72-4e19-9a15-5e59bc9fe2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (1, 384) dtype: float32\n",
      "Qwen local path: /home/manny-buff/projects/capstone/hw-rag/models/Qwen2-VL-2B-Instruct exists: True\n",
      "Graph nodes/edges: 3 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell B: Sanity Probe'\n",
    "Purpose:\n",
    "- Verify core imports.\n",
    "- Run a tiny e5 embedding call to confirm encoder works.\n",
    "- Check that local Qwen path exists (skip heavy model load for now).\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# 'Load run config'\n",
    "import json\n",
    "CFG_RUN = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph/configs/rag_graph_run_config.json\")\n",
    "run_cfg = json.loads(CFG_RUN.read_text())\n",
    "\n",
    "# 'Imports check'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 'Embed a sample query with e5-small-v2'\n",
    "embed_model_id = run_cfg[\"embed_model\"]\n",
    "model = SentenceTransformer(embed_model_id)\n",
    "vec = model.encode([\"hello graph-rag world\"], convert_to_numpy=True)\n",
    "print(\"Embedding shape:\", vec.shape, \"dtype:\", vec.dtype)\n",
    "\n",
    "# 'Confirm local Qwen path exists'\n",
    "qwen_local = Path(run_cfg[\"llm_local_path\"])\n",
    "print(\"Qwen local path:\", qwen_local, \"exists:\", qwen_local.exists())\n",
    "\n",
    "# 'Lightweight graph sanity'\n",
    "G = nx.Graph()\n",
    "G.add_edge(\"doc_A\", \"doc_B\", weight=0.9)\n",
    "G.add_edge(\"doc_B\", \"doc_C\", weight=0.7)\n",
    "print(\"Graph nodes/edges:\", G.number_of_nodes(), G.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3afb4a0-26d5-4307-a215-0e40cd247db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded.\n",
      "CORPUS_ROOT = /home/manny-buff/projects/capstone/hw-rag/data\n",
      "Vector DB path = /home/manny-buff/projects/capstone/week6-rag-graph/artifacts/vdb\n",
      "Extensions searched: ['.csv', '.htm', '.html', '.json', '.md', '.pdf', '.text', '.txt']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell C: Config + Helpers (extended)'\n",
    "- Loads config.\n",
    "- Discovers multiple filetypes.\n",
    "- Extracts text from txt/md/text/pdf/json/csv/html/htm.\n",
    "- Adds a tqdm fallback and silences the common tqdm warning.\n",
    "\"\"\"\n",
    "\n",
    "import os, json, re, math, pickle, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "# Silence noisy tqdm warnings if present\n",
    "warnings.filterwarnings(\"ignore\", message=\".*tqdm.*\")\n",
    "\n",
    "# --- Load run config ---\n",
    "RUN_CFG_PATH = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph/configs/rag_graph_run_config.json\")\n",
    "cfg = json.loads(RUN_CFG_PATH.read_text())\n",
    "\n",
    "CORPUS_ROOT   = Path(cfg[\"corpus_root\"])\n",
    "VDB_DIR       = Path(cfg[\"vector_db_dir\"])\n",
    "EMBED_ID      = cfg[\"embed_model\"]\n",
    "LLM_MODEL_ID  = cfg[\"llm_model_id\"]\n",
    "LLM_LOCAL     = Path(cfg[\"llm_local_path\"])\n",
    "DEVICE        = cfg.get(\"device\", \"cuda\")\n",
    "RETRIEVER_K   = int(cfg.get(\"retriever_k\", 5))\n",
    "HOP_LIMIT     = int(cfg.get(\"hop_limit\", 2))\n",
    "\n",
    "VDB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Optional deps used if available ---\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kw):  # no-op fallback\n",
    "        return x\n",
    "\n",
    "# PDF\n",
    "try:\n",
    "    from pypdf import PdfReader\n",
    "except Exception:\n",
    "    PdfReader = None\n",
    "\n",
    "# HTML\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except Exception:\n",
    "    BeautifulSoup = None\n",
    "\n",
    "# --- File discovery ---\n",
    "EXTS = {\".txt\", \".md\", \".text\", \".pdf\", \".json\", \".csv\", \".html\", \".htm\"}\n",
    "\n",
    "def find_files(root: Path) -> List[Path]:\n",
    "    files = []\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in EXTS:\n",
    "            files.append(p)\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "# --- Loaders by type ---\n",
    "def load_text_plain(fp: Path) -> str:\n",
    "    try:\n",
    "        return fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return fp.read_text(errors=\"ignore\")\n",
    "\n",
    "def load_text_pdf(fp: Path) -> str:\n",
    "    if PdfReader is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        out = []\n",
    "        reader = PdfReader(str(fp))\n",
    "        for page in reader.pages:\n",
    "            out.append(page.extract_text() or \"\")\n",
    "        return \"\\n\".join(out)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def load_text_json(fp: Path) -> str:\n",
    "    try:\n",
    "        obj = json.loads(fp.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "        # Flatten string-like leaf values\n",
    "        def walk(x):\n",
    "            if isinstance(x, dict):\n",
    "                return \" \".join(walk(v) for v in x.values())\n",
    "            if isinstance(x, list):\n",
    "                return \" \".join(walk(v) for v in x)\n",
    "            if isinstance(x, (str, int, float, bool)):\n",
    "                return str(x)\n",
    "            return \"\"\n",
    "        return walk(obj)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def load_text_csv(fp: Path) -> str:\n",
    "    try:\n",
    "        df = pd.read_csv(fp, nrows=10000)  # cap large files\n",
    "        return \" \".join(map(str, df.astype(str).values.ravel().tolist()))\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = pd.read_table(fp, nrows=10000)\n",
    "            return \" \".join(map(str, df.astype(str).values.ravel().tolist()))\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "def load_text_html(fp: Path) -> str:\n",
    "    if BeautifulSoup is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        html = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        return soup.get_text(\" \", strip=True)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "LOADERS = {\n",
    "    \".txt\":  load_text_plain,\n",
    "    \".md\":   load_text_plain,\n",
    "    \".text\": load_text_plain,\n",
    "    \".pdf\":  load_text_pdf,\n",
    "    \".json\": load_text_json,\n",
    "    \".csv\":  load_text_csv,\n",
    "    \".html\": load_text_html,\n",
    "    \".htm\":  load_text_html,\n",
    "}\n",
    "\n",
    "def normalize_ws(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def chunk_text(s: str, max_tokens: int = 180, overlap: int = 30) -> List[str]:\n",
    "    toks = s.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(toks):\n",
    "        j = min(i + max_tokens, len(toks))\n",
    "        chunk = \" \".join(toks[i:j]).strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        if j == len(toks):\n",
    "            break\n",
    "        i = max(0, j - overlap)\n",
    "    return chunks\n",
    "\n",
    "# --- Artifact I/O ---\n",
    "ART_META   = VDB_DIR / \"chunks_meta.parquet\"\n",
    "ART_CHUNKS = VDB_DIR / \"chunks_text.pkl\"\n",
    "ART_FAISS  = VDB_DIR / \"faiss.index\"\n",
    "ART_GRAPH  = VDB_DIR / \"graph.pkl\"\n",
    "\n",
    "import pickle\n",
    "def save_chunks_text(chunks: List[str]):\n",
    "    with open(ART_CHUNKS, \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "\n",
    "def load_chunks_text() -> List[str]:\n",
    "    with open(ART_CHUNKS, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "print(\"Config loaded.\")\n",
    "print(\"CORPUS_ROOT =\", CORPUS_ROOT)\n",
    "print(\"Vector DB path =\", VDB_DIR)\n",
    "print(\"Extensions searched:\", sorted(EXTS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "433bbcc6-c899-452f-934a-5706a183c486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading+Chunking:  71%|███████████████████████████████▊             | 12/17 [00:59<00:12,  2.51s/it]EOF marker not found\n",
      "Loading+Chunking: 100%|█████████████████████████████████████████████| 17/17 [01:01<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 17 | Chunks: 4381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57fbc2756f941ca9ade8d365a847f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: (4381, 384) elapsed_sec: 4.43\n",
      "Saved:\n",
      " - /home/manny-buff/projects/capstone/week6-rag-graph/artifacts/vdb/chunks_meta.parquet\n",
      " - /home/manny-buff/projects/capstone/week6-rag-graph/artifacts/vdb/chunks_text.pkl\n",
      " - /home/manny-buff/projects/capstone/week6-rag-graph/artifacts/vdb/faiss.index\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell D (guarded): Build Embeddings + FAISS index'\n",
    "- Reads/discovers/loads multiple file types.\n",
    "- Chunks and embeds with e5-small-v2.\n",
    "- Builds FAISS IP index (cosine on normalized vectors).\n",
    "- Guards against empty corpus (prints message and returns early).\n",
    "\"\"\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, time, faiss\n",
    "\n",
    "files = find_files(CORPUS_ROOT)\n",
    "records = []\n",
    "chunks_text = []\n",
    "\n",
    "for doc_id, fp in enumerate(tqdm(files, desc=\"Loading+Chunking\")):\n",
    "    loader = LOADERS.get(fp.suffix.lower(), load_text_plain)\n",
    "    raw = loader(fp)\n",
    "    text = normalize_ws(raw)\n",
    "    if not text:\n",
    "        continue\n",
    "    parts = chunk_text(text, max_tokens=180, overlap=30)\n",
    "    for k, ch in enumerate(parts):\n",
    "        records.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_id\": len(chunks_text),\n",
    "            \"path\": str(fp),\n",
    "            \"chunk_idx\": k\n",
    "        })\n",
    "        chunks_text.append(ch)\n",
    "\n",
    "import pandas as pd\n",
    "meta_df = pd.DataFrame(records)\n",
    "print(f\"Docs: {len(files)} | Chunks: {len(chunks_text)}\")\n",
    "\n",
    "# Guard: no chunks → stop gracefully\n",
    "if len(chunks_text) == 0:\n",
    "    print(\"No chunks found. Please confirm corpus file types and that loaders extracted text.\")\n",
    "    # Tip for debugging: run the shell probe to see extensions/counts.\n",
    "    raise SystemExit\n",
    "\n",
    "# Embed\n",
    "model = SentenceTransformer(EMBED_ID)\n",
    "t0 = time.time()\n",
    "emb = model.encode(\n",
    "    chunks_text,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "print(\"Embeddings:\", emb.shape, \"elapsed_sec:\", round(time.time()-t0, 2))\n",
    "\n",
    "# Build FAISS\n",
    "dim = emb.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(emb.astype(np.float32))\n",
    "\n",
    "# Save artifacts\n",
    "meta_df.to_parquet(ART_META, index=False)\n",
    "save_chunks_text(chunks_text)\n",
    "faiss.write_index(index, str(ART_FAISS))\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", ART_META)\n",
    "print(\" -\", ART_CHUNKS)\n",
    "print(\" -\", ART_FAISS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939e1843-43da-48ca-bad9-62016dbf1d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built.\n",
      "Nodes: 4381 Edges: 46574\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell E: Build Similarity Graph'\n",
    "Purpose:\n",
    "- Create a lightweight graph of chunk relationships using top-N cosine neighbors.\n",
    "- Collapses edges to doc-level (optional) or keeps chunk-level. We'll keep chunk-level for precision.\n",
    "- Save graph.pkl for later Multi-Hop traversal.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np, networkx as nx, faiss, math\n",
    "\n",
    "# Params for graph density\n",
    "TOP_NEIGHBORS = max(10, RETRIEVER_K * 3)  # small multiple of retriever_k\n",
    "\n",
    "# Load index and chunks\n",
    "index = faiss.read_index(str(ART_FAISS))\n",
    "chunks = load_chunks_text()\n",
    "meta  = pd.read_parquet(ART_META)\n",
    "\n",
    "# Query each vector against index to get neighbors (excluding self)\n",
    "D, I = index.search(emb.astype(np.float32), TOP_NEIGHBORS + 1)\n",
    "\n",
    "G = nx.Graph()\n",
    "for row_idx, nbrs in enumerate(I):\n",
    "    src = int(row_idx)\n",
    "    for rank, nb in enumerate(nbrs):\n",
    "        if nb == -1 or nb == src: \n",
    "            continue\n",
    "        w = float(D[row_idx, rank])\n",
    "        if w <= 0: \n",
    "            continue\n",
    "        # Add undirected edge with weight=max(existing,w)\n",
    "        if G.has_edge(src, nb):\n",
    "            if w > G[src][nb].get(\"weight\", 0.0):\n",
    "                G[src][nb][\"weight\"] = w\n",
    "        else:\n",
    "            G.add_edge(src, nb, weight=w)\n",
    "\n",
    "# Persist graph\n",
    "with open(ART_GRAPH, \"wb\") as f:\n",
    "    pickle.dump(G, f)\n",
    "\n",
    "print(\"Graph built.\")\n",
    "print(\"Nodes:\", G.number_of_nodes(), \"Edges:\", G.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb4d72e-a933-48fb-aadf-b356e294ac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k retrieved:\n",
      "[1] score=0.822 | /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#1033 :: For inspecting a car's fi:ont and rear lights, p. 29, \"Night moves\" > Minor tUe for mounting on the ceil- ing of a closet to see what is …\n",
      "[2] score=0.815 | /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#1008 :: for a child's dresser, p. 21, \"Playful pulls\" Aluminum foil >■ Wrapped around pillows, to keep cats off the sofa, p. 41, \"Stay off the …\n",
      "[3] score=0.815 | /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#1030 :: pour before completion, p. 258, \"Easy pour\" >■ To prevent a toilet bowl from \"sweating\" m humid weather, p. 173, \"Bathroom condensation\" …\n",
      "[4] score=0.815 | /home/manny-buff/projects/capstone/hw-rag/data/the-complete-idiots-guide-to-simple-home-repair.pdf | chunk#26 :: motion. Warmest thanks are also due to Lynn Northrup, Jan Lynn, and Ross Patty, whose thoughtful suggestions and unflagging attention made …\n",
      "[5] score=0.812 | /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#1036 :: \"Do the pan-pan\" Pants hanger, wood >• To store paper bags, p. 33, \"How to brown bag it?\" >• Filled with manure and j water, for …\n",
      "\n",
      "Graph neighbors (first 10) of top chunk: 1033\n",
      "Neighbors: [1012, 1019, np.int64(1051), np.int64(1042), np.int64(1034), np.int64(1043), np.int64(1032), np.int64(1023), np.int64(1048), np.int64(1018)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell F: Validate'\n",
    "Purpose:\n",
    "- Issue a sample query to FAISS, print top-k chunk previews\n",
    "- Show 1-step neighbors in the graph for the top hit (sanity for Multi-Hop)\n",
    "\"\"\"\n",
    "\n",
    "import textwrap, faiss, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "index = faiss.read_index(str(ART_FAISS))\n",
    "meta  = pd.read_parquet(ART_META)\n",
    "chunks = load_chunks_text()\n",
    "enc   = SentenceTransformer(EMBED_ID)\n",
    "\n",
    "query = \"Briefly summarize the core topic of this corpus.\"\n",
    "qv = enc.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "D, I = index.search(qv.astype(np.float32), RETRIEVER_K)\n",
    "\n",
    "print(\"Top-k retrieved:\")\n",
    "for rank, cid in enumerate(I[0]):\n",
    "    doc = meta.loc[meta[\"chunk_id\"]==cid].iloc[0]\n",
    "    preview = textwrap.shorten(chunks[cid], width=140, placeholder=\" …\")\n",
    "    print(f\"[{rank+1}] score={D[0,rank]:.3f} | {doc['path']} | chunk#{doc['chunk_idx']} :: {preview}\")\n",
    "\n",
    "# Graph neighbor preview for top hit\n",
    "top_chunk = int(I[0,0])\n",
    "print(\"\\nGraph neighbors (first 10) of top chunk:\", top_chunk)\n",
    "with open(ART_GRAPH, \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "nbrs = list(G.neighbors(top_chunk))[:10]\n",
    "print(\"Neighbors:\", nbrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef0811e-362f-4e56-b3ab-1351b356ea44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded meta: (4381, 4) | chunks: 4381\n",
      "FAISS dims: 384 | Graph nodes/edges: 4381 46574\n",
      "LLM local path exists: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell G: Load config + artifacts + embedder'\n",
    "Purpose:\n",
    "- Read run config (same JSON as Week6-1).\n",
    "- Load FAISS, graph, metadata, and chunks.\n",
    "- Initialize e5-small-v2 embedder.\n",
    "\"\"\"\n",
    "\n",
    "import json, pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss, networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 'Paths'\n",
    "ROOT = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph\")\n",
    "CFG_RUN = ROOT / \"configs\" / \"rag_graph_run_config.json\"\n",
    "VDB     = ROOT / \"artifacts\" / \"vdb\"\n",
    "\n",
    "# 'Artifacts'\n",
    "ART_META   = VDB / \"chunks_meta.parquet\"\n",
    "ART_CHUNKS = VDB / \"chunks_text.pkl\"\n",
    "ART_FAISS  = VDB / \"faiss.index\"\n",
    "ART_GRAPH  = VDB / \"graph.pkl\"\n",
    "\n",
    "# 'Load config'\n",
    "cfg = json.loads(CFG_RUN.read_text())\n",
    "CORPUS_ROOT  = Path(cfg[\"corpus_root\"])\n",
    "EMBED_ID     = cfg[\"embed_model\"]              # e.g., 'intfloat/e5-small-v2'\n",
    "LLM_MODEL_ID = cfg[\"llm_model_id\"]             # e.g., 'Qwen/Qwen2.5-VL-3B-Instruct'\n",
    "LLM_LOCAL    = Path(cfg[\"llm_local_path\"])     # local Qwen path\n",
    "RETRIEVER_K  = int(cfg.get(\"retriever_k\", 5))\n",
    "HOP_LIMIT    = int(cfg.get(\"hop_limit\", 2))\n",
    "\n",
    "# 'Load artifacts'\n",
    "meta   = pd.read_parquet(ART_META)\n",
    "chunks = pickle.loads(ART_CHUNKS.read_bytes())\n",
    "index  = faiss.read_index(str(ART_FAISS))\n",
    "with open(ART_GRAPH, \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "# 'Embedder'\n",
    "embedder = SentenceTransformer(EMBED_ID)\n",
    "\n",
    "print(\"Loaded meta:\", meta.shape, \"| chunks:\", len(chunks))\n",
    "print(\"FAISS dims:\", index.d, \"| Graph nodes/edges:\", G.number_of_nodes(), G.number_of_edges())\n",
    "print(\"LLM local path exists:\", LLM_LOCAL.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9451faf6-f236-434a-98aa-c0284529fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "'Cell H: Retrieval + Graph Expansion + Context Builder'\n",
    "Purpose:\n",
    "- Provide dense retriever (FAISS).\n",
    "- Provide neighbor expansion up to HOP_LIMIT with breadth cap.\n",
    "- Consolidate chunks into a prompt context (size-bounded).\n",
    "\"\"\"\n",
    "\n",
    "import math, textwrap\n",
    "from typing import List, Set, Dict\n",
    "\n",
    "def dense_retrieve(query: str, top_k: int) -> List[int]:\n",
    "    # 'Encode query and search FAISS; return chunk IDs'\n",
    "    q = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    D, I = index.search(q.astype(np.float32), top_k)\n",
    "    return [int(x) for x in I[0]]\n",
    "\n",
    "def expand_via_graph(seed_ids: List[int], hop_limit: int, per_seed_cap: int = 20, global_cap: int = 200) -> List[int]:\n",
    "    # 'Expand neighbors up to hop_limit; cap breadth; dedupe; return chunk IDs'\n",
    "    visited: Set[int] = set(int(s) for s in seed_ids)\n",
    "    frontier: Set[int] = set(visited)\n",
    "    for hop in range(hop_limit):\n",
    "        next_frontier: Set[int] = set()\n",
    "        for node in list(frontier):\n",
    "            nbrs = list(G.neighbors(node))\n",
    "            # limit per-seed to avoid explosion\n",
    "            for nb in nbrs[:per_seed_cap]:\n",
    "                nb = int(nb)  # guard np.int64\n",
    "                if nb not in visited:\n",
    "                    next_frontier.add(nb)\n",
    "        frontier = next_frontier\n",
    "        visited.update(frontier)\n",
    "        if len(visited) >= global_cap:\n",
    "            break\n",
    "    return list(visited)\n",
    "\n",
    "def build_context(chunk_ids: List[int], max_chars: int = 4000) -> str:\n",
    "    # 'Join snippets with lightweight headers; stop at char budget'\n",
    "    out = []\n",
    "    size = 0\n",
    "    for cid in chunk_ids:\n",
    "        row = meta.loc[meta[\"chunk_id\"] == cid]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        path = row.iloc[0][\"path\"]\n",
    "        idx  = row.iloc[0][\"chunk_idx\"]\n",
    "        snippet = textwrap.shorten(chunks[cid], width=360, placeholder=\" …\")\n",
    "        block = f\"[SOURCE] {path} | chunk#{idx}\\n{snippet}\\n\"\n",
    "        if size + len(block) > max_chars:\n",
    "            break\n",
    "        out.append(block)\n",
    "        size += len(block)\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def retrieve_expand_context(query: str, top_k: int = None, hop_limit: int = None, per_seed_cap: int = 20, global_cap: int = 200, max_chars: int = 4000):\n",
    "    # 'End-to-end helper: dense retrieval → graph expand → context'\n",
    "    if top_k is None:    top_k    = RETRIEVER_K\n",
    "    if hop_limit is None: hop_limit = HOP_LIMIT\n",
    "\n",
    "    seeds = dense_retrieve(query, top_k)\n",
    "    expanded = expand_via_graph(seeds, hop_limit, per_seed_cap=per_seed_cap, global_cap=global_cap)\n",
    "    # order: seeds first, then expanded (stable, unique)\n",
    "    ordered = []\n",
    "    seen = set()\n",
    "    for x in seeds + expanded:\n",
    "        if x not in seen:\n",
    "            ordered.append(x)\n",
    "            seen.add(x)\n",
    "    context = build_context(ordered, max_chars=max_chars)\n",
    "    return seeds, ordered, context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ada504c1-b924-456e-9e51-3474f06ca155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QwenAnswerer (VL-aware, warning-free) ready. Call qwen.load() before answering.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell I: Qwen Answer Synthesis (VL-aware, warning-free)'\n",
    "- Uses AutoProcessor(use_fast=False) to silence the fast/slow warning.\n",
    "- Uses AutoModelForImageTextToText (replaces deprecated AutoModelForVision2Seq).\n",
    "- Uses chat template when available for better instruction adherence.\n",
    "- Deterministic generation: no temperature/top_p/top_k; do_sample=False.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoConfig, AutoProcessor, AutoModelForImageTextToText,\n",
    "    AutoTokenizer, AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "class QwenAnswerer:\n",
    "    def __init__(self, local_dir: Path, model_id: str, device: str = \"cuda\"):\n",
    "        self.local_dir = local_dir\n",
    "        self.model_id  = model_id\n",
    "        self.device    = device\n",
    "        self.is_vl     = False\n",
    "        self.processor = None\n",
    "        self.tokenizer = None\n",
    "        self.model     = None\n",
    "\n",
    "    def _load_from(self, src: str):\n",
    "        cfg = AutoConfig.from_pretrained(src, trust_remote_code=True)\n",
    "        mtype = getattr(cfg, \"model_type\", \"\").lower()\n",
    "\n",
    "        if \"vl\" in mtype:  # qwen2_vl / qwen2_5_vl\n",
    "            self.is_vl = True\n",
    "            # use_fast=False: silences the processor warning and preserves old behavior\n",
    "            self.processor = AutoProcessor.from_pretrained(src, trust_remote_code=True, use_fast=False)\n",
    "            self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "                src, trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            self.is_vl = False\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(src, trust_remote_code=True)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                src, trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "\n",
    "    def load(self):\n",
    "        last_err = None\n",
    "        sources = []\n",
    "        if self.local_dir.exists():\n",
    "            sources.append(str(self.local_dir))\n",
    "        sources.append(self.model_id)\n",
    "        for src in sources:\n",
    "            try:\n",
    "                self._load_from(src)\n",
    "                return\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                self.processor = self.tokenizer = self.model = None\n",
    "        raise RuntimeError(f\"Failed to load Qwen (VL-aware). Last error: {last_err}\")\n",
    "\n",
    "    def _format_prompt(self, question: str, context: str) -> dict:\n",
    "        \"\"\"\n",
    "        Returns a dict with tokenization-ready inputs.\n",
    "        Uses chat template if available for better instruction following.\n",
    "        \"\"\"\n",
    "        system_msg = \"You are a concise home-repair RAG assistant. Use ONLY the provided context. If missing info, say so.\"\n",
    "        user_msg   = f\"Question:\\n{question}\\n\\nContext:\\n{context}\\n\\nRespond concisely and cite key source file names.\"\n",
    "\n",
    "        if self.is_vl:\n",
    "            tok = self.processor.tokenizer\n",
    "        else:\n",
    "            tok = self.tokenizer\n",
    "\n",
    "        apply_chat = getattr(tok, \"apply_chat_template\", None)\n",
    "        if callable(apply_chat):\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_msg},\n",
    "                {\"role\": \"user\",   \"content\": user_msg}\n",
    "            ]\n",
    "            prompt_text = apply_chat(messages, tokenize=False, add_generation_prompt=True)\n",
    "        else:\n",
    "            prompt_text = f\"{system_msg}\\n\\n{user_msg}\"\n",
    "\n",
    "        return {\"text\": prompt_text}\n",
    "\n",
    "    def answer(self, question: str, context: str, max_new_tokens: int = 240) -> str:\n",
    "        assert self.model is not None, \"Model not loaded\"\n",
    "        prompt_dict = self._format_prompt(question, context)\n",
    "\n",
    "        if self.is_vl:\n",
    "            # text-only path via processor; deterministic generation (no sampling flags)\n",
    "            inputs = self.processor(**prompt_dict, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                out = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            text = self.processor.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        else:\n",
    "            inputs = self.tokenizer(prompt_dict[\"text\"], return_tensors=\"pt\").to(self.model.device)\n",
    "            with torch.no_grad():\n",
    "                out = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            text = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "qwen = QwenAnswerer(LLM_LOCAL, LLM_MODEL_ID)\n",
    "print(\"QwenAnswerer (VL-aware, warning-free) ready. Call qwen.load() before answering.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "127fd223-1d82-4670-9d50-a7c06f0ec8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds: [1203, 528, 287, 507, 1199] … total: 5\n",
      "Expanded set size: 813\n",
      "Context preview:\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/A Dirty Guide to a Clean Home _ Housekeeping Hacks You Cant Live Without.pdf | chunk#107\n",
      "dusty and are a pain to clean. There is no need for a fancy tool to clean them, your hand is the perfect solution. The easiest way is just to grab a sock, slightly dampen it, put it over your hand, grasp each slat, and glide it along from one side to the other. If your blinds need a super deep clean, you can give them a bath. Yes, that’s right. If the …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.p…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079acaa9ba454018a48f4efb3b08ed06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Answer ===\n",
      " system\n",
      "You are a concise home-repair RAG assistant. Use ONLY the provided context. If missing info, say so.\n",
      "user\n",
      "Question:\n",
      "How can I stop a toilet tank from sweating in humid weather, and what simple materials do I need?\n",
      "\n",
      "Context:\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/A Dirty Guide to a Clean Home _ Housekeeping Hacks You Cant Live Without.pdf | chunk#107\n",
      "dusty and are a pain to clean. There is no need for a fancy tool to clean them, your hand is the perfect solution. The easiest way is just to grab a sock, slightly dampen it, put it over your hand, grasp each slat, and glide it along from one side to the other. If your blinds need a super deep clean, you can give them a bath. Yes, that’s right. If the …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#528\n",
      "Royal flush. Try replacing an old toilet writh a new low- flow model. A family of four can save over 20,000 gallons of water each year. Ask your plumber about the models that work best. No goldbricldng. Don t put a brick in your toilet tank to save water by taking up space; it wrtll eventually disintegrate, causing the toilet to leak. Instead, use a water- …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#287\n",
      "absorbing it, put a layer of clay beneath the topsoil or sod. Cracks are big trouble. Open cracks along the base- ment floor, or vertical cracks along a basement wall that are wider at the top than at the bottom, indicate a possible problem with the footings beneath the foun- dation. Consult a structural engmeer for advice Clear drainpipes. Roof drainpipes …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#507\n",
      "low speed. For a laminate countertop. use a hole saw. With an adjustable wrench, mount the faucet according to the directions. Be careful: the sink hole will be very sharp. 2 Choose a convenient place for the filter system: in the sink cabinet, for instance, or in the basement below. Mount the unit on a solid surface with screws through mounting holes, or …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/A Dirty Guide to a Clean Home _ Housekeeping Hacks You Cant Live Without.pdf | chunk#103\n",
      "the ceiling? That’s probably an exhaust fan. They are an important part of the home’s ventilation system. They suck up moisture, humidity that can lead to mold and mildew growth, and eliminate odor. They exist almost exclusively in bathrooms, which get steamy, but if you live in an apartment building, you might also have one in your kitchen. To clean any …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/Complete home repair  with 350 projects and 2300 photos.pdf | chunk#577\n",
      "on each side to allow for expansion. Attach the sheathing to the nailing strips and studs, using 2Y4\" deck screws driven every 12\" (photo C). Tools: Hammer, circular saw, tape measure, chalk line, pry bar, drill. Materials: sheathing, 2x4 lumber, 3\" deck screws, 21/,\" deck screws. Leave a 14\" expansion gap around the new sheathing and install with 214\" …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/Complete home repair  with 350 projects and 2300 photos.pdf | chunk#578\n",
      "jointing tool. Materials: Mortar mix, concrete for¬ tifier, mortar pigment. determine whether they’re sound. Tuckpoint deteriorated joints. Spalling occurs when trapped moisture is exposed to repeated freeze and thaw cycles, exerting enough directional pressure to fracture Tuckpoint joints by removing cracked, damaged mortar and filling the joints with …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#14\n",
      "the Off position to shut off the power On a cartridge- Pull out the plastic boxes holding type box the cartridge fuses. On the main circuit Flip the one or two main switches breaker box to Off. On other circuit breaker boxes NATURAL GAS Flip all switches to Off CAtJTION: If you smell gas, open the windows and shut off the main gas valve. Do not light a …\n",
      "\n",
      "\n",
      "Respond concisely and cite key source file names.\n",
      "assistant\n",
      "To stop a toilet tank from sweating in humid weather, you can use a damp sock to slide along the slats. You can also use a layer of clay beneath the topsoil or sod to absorb moisture. To clean any cracks in the basement floor or wall, consult a structural engineer. To clean a laminate countertop, use a hole saw and attach the sheathing to the nailing strips and studs with 2x4 lumber and 3\" deck screws. To clean exhaust fans, install a filter system in the sink cabinet or basement below. \n",
      "\n",
      "Latency (s): 1.05\n",
      "Ablation row appended to /home/manny-buff/projects/capstone/week6-rag-graph/artifacts/ablation_results_graph.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell J: Run query + log ablation row'\n",
    "Purpose:\n",
    "- Execute retrieve → expand → context build.\n",
    "- Load Qwen and generate answer.\n",
    "- Append an ablation row to CSV.\n",
    "\"\"\"\n",
    "\n",
    "import time, csv, os\n",
    "ABL_PATH = ROOT / \"artifacts\" / \"ablation_results_graph.csv\"\n",
    "\n",
    "query = \"How can I stop a toilet tank from sweating in humid weather, and what simple materials do I need?\"\n",
    "seeds, expanded, context = retrieve_expand_context(query, top_k=RETRIEVER_K, hop_limit=HOP_LIMIT, per_seed_cap=20, global_cap=200, max_chars=4000)\n",
    "\n",
    "print(\"Seeds:\", seeds[:10], \"… total:\", len(seeds))\n",
    "print(\"Expanded set size:\", len(expanded))\n",
    "print(\"Context preview:\\n\", context[:600], \"…\", sep=\"\")\n",
    "\n",
    "\"\"\"\n",
    "Small patch: keep Cell J logic as-is, but no sampling flags are passed by QwenAnswerer now.\n",
    "Just re-run this cell after re-running Cell I and calling qwen.load().\n",
    "\"\"\"\n",
    "\n",
    "# (re-run as before)\n",
    "qwen.load()\n",
    "t0 = time.time()\n",
    "answer = qwen.answer(query, context, max_new_tokens=256)\n",
    "elapsed = round(time.time() - t0, 2)\n",
    "\n",
    "print(\"\\n=== Answer ===\\n\", answer, \"\\n\")\n",
    "print(\"Latency (s):\", elapsed)\n",
    "# ablation append stays unchanged\n",
    "\n",
    "# 'Log ablation row (accuracy left blank for manual scoring later)'\n",
    "row = {\n",
    "    \"variant\": \"dense+graph\",\n",
    "    \"retriever_k\": RETRIEVER_K,\n",
    "    \"hop_limit\": HOP_LIMIT,\n",
    "    \"accuracy\": \"\",\n",
    "    \"notes\": f\"seeds={len(seeds)} expanded={len(expanded)} latency_s={elapsed}\"\n",
    "}\n",
    "\n",
    "# Append row\n",
    "header = [\"variant\", \"retriever_k\", \"hop_limit\", \"accuracy\", \"notes\"]\n",
    "file_exists = ABL_PATH.exists()\n",
    "with open(ABL_PATH, \"a\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=header)\n",
    "    if not file_exists:\n",
    "        w.writeheader()\n",
    "    w.writerow(row)\n",
    "\n",
    "print(f\"Ablation row appended to {ABL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83257a49-4cc3-459c-9e31-5ab292c6a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 6 — Part 2: Multi-Hop QA (Summary)\n",
    "'''\n",
    "**Retriever**: FAISS (cosine on normalized e5-small-v2)  \n",
    "**Graph Expansion**: neighbor hops = `HOP_LIMIT` (default 2), per-seed cap = 20, global cap = 200  \n",
    "**LLM**: Qwen-VL (VL-aware loader, deterministic `do_sample=False`)  \n",
    "**Prompting**: Chat template when available; system+user roles; context-only constraint\n",
    "\n",
    "### Pipeline\n",
    "1. **Dense Retrieval** → top-K chunk IDs.\n",
    "2. **Graph Expansion** → add neighbors (bounded breadth), dedupe, seeds prioritized.\n",
    "3. **Context Build** → path+chunk headers with short previews, size-bounded.\n",
    "4. **Answer Synthesis** → Qwen-VL text-only path; concise answer with file citations.\n",
    "\n",
    "### Validations\n",
    "- Seeds/Expanded sizes printed.\n",
    "- Answer generated without processor/deprecation/generation warnings.\n",
    "- Ablation row appended to `artifacts/ablation_results_graph.csv`.\n",
    "\n",
    "**Determinism**: No sampling; repeated runs on the same artifacts/config should match.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deea3254-25e4-43ea-8006-1974e191fb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /home/manny-buff/projects/capstone/week6-rag-graph/artifacts/Report_snippets_Wk6_1_2.md\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Report Retrieval Script #1 (Week6-1 & Week6-2)'\n",
    "Purpose:\n",
    "- Create or update a single markdown file summarizing environment, config,\n",
    "  index/graph stats, and ablation results for Parts 1 & 2.\n",
    "- Output: artifacts/Report_snippets_Wk6_1_2.md\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import json, pickle, pandas as pd\n",
    "\n",
    "ROOT = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph\")\n",
    "OUT  = ROOT / \"artifacts\" / \"Report_snippets_Wk6_1_2.md\"\n",
    "\n",
    "CFG_ENV = ROOT / \"configs\" / \"env_rag_graph.json\"\n",
    "CFG_RUN = ROOT / \"configs\" / \"rag_graph_run_config.json\"\n",
    "VDB     = ROOT / \"artifacts\" / \"vdb\"\n",
    "\n",
    "ART_META   = VDB / \"chunks_meta.parquet\"\n",
    "ART_FAISS  = VDB / \"faiss.index\"\n",
    "ART_GRAPH  = VDB / \"graph.pkl\"\n",
    "ABL_CSV    = ROOT / \"artifacts\" / \"ablation_results_graph.csv\"\n",
    "\n",
    "# Load pieces (best-effort)\n",
    "env_info = json.loads(CFG_ENV.read_text()) if CFG_ENV.exists() else {}\n",
    "run_cfg  = json.loads(CFG_RUN.read_text()) if CFG_RUN.exists() else {}\n",
    "meta_df  = pd.read_parquet(ART_META) if ART_META.exists() else pd.DataFrame()\n",
    "graph_n  = graph_e = None\n",
    "if ART_GRAPH.exists():\n",
    "    with open(ART_GRAPH, \"rb\") as f:\n",
    "        G = pickle.load(f)\n",
    "        graph_n, graph_e = G.number_of_nodes(), G.number_of_edges()\n",
    "abl_df = pd.read_csv(ABL_CSV) if ABL_CSV.exists() else pd.DataFrame()\n",
    "\n",
    "# Compose markdown\n",
    "lines = []\n",
    "lines.append(\"# Week 6 — Report Snippets (Parts 1 & 2)\\n\")\n",
    "lines.append(\"## Environment\")\n",
    "lines.append(f\"- Python venv: `{env_info.get('python_venv','')}`\")\n",
    "lines.append(f\"- Python: `{env_info.get('python_version','')}`\")\n",
    "lines.append(f\"- GPU: `{env_info.get('nvidia_smi','')}`\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Run Config\")\n",
    "for k in [\"corpus_root\",\"embed_model\",\"llm_model_id\",\"llm_local_path\",\"device\",\"retriever_k\",\"hop_limit\"]:\n",
    "    lines.append(f\"- {k}: `{run_cfg.get(k,'')}`\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Artifacts\")\n",
    "lines.append(f\"- Meta rows (chunks): {len(meta_df) if not meta_df.empty else 0}\")\n",
    "lines.append(f\"- FAISS index: {'present' if ART_FAISS.exists() else 'missing'}\")\n",
    "if graph_n is not None:\n",
    "    lines.append(f\"- Graph nodes: {graph_n} | edges: {graph_e}\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Ablation Results (head)\")\n",
    "if not abl_df.empty:\n",
    "    lines.append(abl_df.head(5).to_markdown(index=False))\n",
    "else:\n",
    "    lines.append(\"_No ablation rows yet._\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Ablation Results (tail)\")\n",
    "if not abl_df.empty:\n",
    "    lines.append(abl_df.tail(5).to_markdown(index=False))\n",
    "else:\n",
    "    lines.append(\"_No ablation rows yet._\")\n",
    "\n",
    "OUT.write_text(\"\\n\".join(lines))\n",
    "print(f\"Wrote: {OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447bfcb-2db5-4ad4-add1-9e32ecb2c8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (core-rag)",
   "language": "python",
   "name": "core-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
