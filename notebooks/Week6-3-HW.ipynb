{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e824e7c-c5aa-4cab-8424-8a15c7f7313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Execution Notes (Read First)\n",
    "'''\n",
    "The early cells that wrote config / launched Streamlit were experimental and failed on this machine.\n",
    "✅ Use **Cell K** to write `app/app_rag.py`.  \n",
    "✅ Launch Streamlit **from terminal** as documented.  \n",
    "⛔ Do **not** run the old launcher/config cells below; they are kept for traceability.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f2519a2-2ce5-4127-a274-5b1852e7f865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manny-buff/venvs/core-rag/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment probe written to: /home/manny-buff/projects/capstone/week6-rag-graph/configs/env_rag_graph.json\n",
      "{\n",
      "  \"python_venv\": \"/home/manny-buff/venvs/core-rag\",\n",
      "  \"python_version\": \"3.11.9\",\n",
      "  \"platform\": \"Linux-6.14.0-33-generic-x86_64-with-glibc2.39\",\n",
      "  \"cuda_visible_devices\": null,\n",
      "  \"nvidia_smi\": \"NVIDIA GeForce RTX 4080, 580.65.06, 16376 MiB\",\n",
      "  \"which_python\": \"/home/manny-buff/venvs/core-rag/bin/python\",\n",
      "  \"pip_freeze_head\": \"accelerate==1.10.1\\nacres==0.5.0\\naiofiles==24.1.0\\naiohappyeyeballs==2.6.1\\naiohttp==3.12.15\\naiosignal==1.4.0\\naiosqlite==0.21.0\\naltair==5.5.0\\nannotated-types==0.7.0\\nanyio==4.10.0\\nargon2-cffi==25.1.0\\nargon2-cffi-bindings==25.1.0\\narrow==1.3.0\\nasttokens==3.0.0\\nasync-lru==2.0.5\\nattrs==25.3.0\\nav==15.1.0\\nbabel==2.17.0\\nbackoff==2.2.1\\nbanks==2.2.0\",\n",
      "  \"packages\": {\n",
      "    \"numpy\": \"2.2.1\",\n",
      "    \"pandas\": \"2.2.3\",\n",
      "    \"networkx\": \"3.3\",\n",
      "    \"sentence_transformers\": \"3.0.1\",\n",
      "    \"transformers\": \"4.56.2\",\n",
      "    \"accelerate\": \"1.10.1\",\n",
      "    \"faiss\": \"1.10.0\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell A: Environment Probe'\n",
    "Purpose:\n",
    "- Detect Python version, CUDA toolkits (if available), GPU info, and key package versions.\n",
    "- Save a merged record into configs/env_rag_graph.json for reproducibility.\n",
    "\n",
    "Notes:\n",
    "- Uses only standard libs + minimal imports to avoid heavy loads here.\n",
    "\"\"\"\n",
    "\n",
    "import json, os, sys, subprocess, shutil, platform\n",
    "from pathlib import Path\n",
    "\n",
    "# 'Paths and files' - adjust only if your project layout changes\n",
    "ROOT = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph\")\n",
    "CFG  = ROOT / \"configs\" / \"env_rag_graph.json\"\n",
    "\n",
    "def cmd_out(args):\n",
    "    # 'Run a shell command safely and return stdout text'\n",
    "    try:\n",
    "        return subprocess.check_output(args, stderr=subprocess.STDOUT, text=True).strip()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "# 'Collect environment info'\n",
    "info = {\n",
    "    \"python_venv\": str(Path.home() / \"venvs\" / \"core-rag\"),\n",
    "    \"python_version\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cuda_visible_devices\": os.environ.get(\"CUDA_VISIBLE_DEVICES\", None),\n",
    "    \"nvidia_smi\": cmd_out([\"bash\", \"-lc\", \"nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv,noheader\"]),\n",
    "    \"which_python\": cmd_out([\"bash\", \"-lc\", \"which python\"]),\n",
    "    \"pip_freeze_head\": cmd_out([\"bash\", \"-lc\", \"pip freeze | head -n 20\"])\n",
    "}\n",
    "\n",
    "# 'Key packages versions' - quick imports to record versions\n",
    "versions = {}\n",
    "for pkg in [\"numpy\", \"pandas\", \"networkx\", \"sentence_transformers\", \"transformers\", \"accelerate\", \"faiss\"]:\n",
    "    try:\n",
    "        mod = __import__(pkg)\n",
    "        versions[pkg] = getattr(mod, \"__version__\", \"unknown\")\n",
    "    except Exception as e:\n",
    "        versions[pkg] = f\"not importable: {e}\"\n",
    "\n",
    "info[\"packages\"] = versions\n",
    "\n",
    "# 'Merge with existing json'\n",
    "CFG.parent.mkdir(parents=True, exist_ok=True)\n",
    "existing = {}\n",
    "if CFG.exists():\n",
    "    try:\n",
    "        existing = json.loads(CFG.read_text())\n",
    "    except Exception:\n",
    "        existing = {}\n",
    "\n",
    "existing.update(info)\n",
    "CFG.write_text(json.dumps(existing, indent=2))\n",
    "\n",
    "print(\"Environment probe written to:\", CFG)\n",
    "print(json.dumps(info, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "857c4935-ca72-4e19-9a15-5e59bc9fe2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (1, 384) dtype: float32\n",
      "Qwen local path: /home/manny-buff/projects/capstone/hw-rag/models/Qwen2-VL-2B-Instruct exists: True\n",
      "Graph nodes/edges: 3 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell B: Sanity Probe'\n",
    "Purpose:\n",
    "- Verify core imports.\n",
    "- Run a tiny e5 embedding call to confirm encoder works.\n",
    "- Check that local Qwen path exists (skip heavy model load for now).\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# 'Load run config'\n",
    "import json\n",
    "CFG_RUN = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph/configs/rag_graph_run_config.json\")\n",
    "run_cfg = json.loads(CFG_RUN.read_text())\n",
    "\n",
    "# 'Imports check'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 'Embed a sample query with e5-small-v2'\n",
    "embed_model_id = run_cfg[\"embed_model\"]\n",
    "model = SentenceTransformer(embed_model_id)\n",
    "vec = model.encode([\"hello graph-rag world\"], convert_to_numpy=True)\n",
    "print(\"Embedding shape:\", vec.shape, \"dtype:\", vec.dtype)\n",
    "\n",
    "# 'Confirm local Qwen path exists'\n",
    "qwen_local = Path(run_cfg[\"llm_local_path\"])\n",
    "print(\"Qwen local path:\", qwen_local, \"exists:\", qwen_local.exists())\n",
    "\n",
    "# 'Lightweight graph sanity'\n",
    "G = nx.Graph()\n",
    "G.add_edge(\"doc_A\", \"doc_B\", weight=0.9)\n",
    "G.add_edge(\"doc_B\", \"doc_C\", weight=0.7)\n",
    "print(\"Graph nodes/edges:\", G.number_of_nodes(), G.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3afb4a0-26d5-4307-a215-0e40cd247db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded.\n",
      "CORPUS_ROOT = /home/manny-buff/projects/capstone/hw-rag/data\n",
      "Vector DB path = /home/manny-buff/projects/capstone/week6-rag-graph/artifacts/vdb\n",
      "Extensions searched: ['.csv', '.htm', '.html', '.json', '.md', '.pdf', '.text', '.txt']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell C: Config + Helpers (extended)'\n",
    "- Loads config.\n",
    "- Discovers multiple filetypes.\n",
    "- Extracts text from txt/md/text/pdf/json/csv/html/htm.\n",
    "- Adds a tqdm fallback and silences the common tqdm warning.\n",
    "\"\"\"\n",
    "\n",
    "import os, json, re, math, pickle, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "# Silence noisy tqdm warnings if present\n",
    "warnings.filterwarnings(\"ignore\", message=\".*tqdm.*\")\n",
    "\n",
    "# --- Load run config ---\n",
    "RUN_CFG_PATH = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph/configs/rag_graph_run_config.json\")\n",
    "cfg = json.loads(RUN_CFG_PATH.read_text())\n",
    "\n",
    "CORPUS_ROOT   = Path(cfg[\"corpus_root\"])\n",
    "VDB_DIR       = Path(cfg[\"vector_db_dir\"])\n",
    "EMBED_ID      = cfg[\"embed_model\"]\n",
    "LLM_MODEL_ID  = cfg[\"llm_model_id\"]\n",
    "LLM_LOCAL     = Path(cfg[\"llm_local_path\"])\n",
    "DEVICE        = cfg.get(\"device\", \"cuda\")\n",
    "RETRIEVER_K   = int(cfg.get(\"retriever_k\", 5))\n",
    "HOP_LIMIT     = int(cfg.get(\"hop_limit\", 2))\n",
    "\n",
    "VDB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Optional deps used if available ---\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    def tqdm(x, **kw):  # no-op fallback\n",
    "        return x\n",
    "\n",
    "# PDF\n",
    "try:\n",
    "    from pypdf import PdfReader\n",
    "except Exception:\n",
    "    PdfReader = None\n",
    "\n",
    "# HTML\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except Exception:\n",
    "    BeautifulSoup = None\n",
    "\n",
    "# --- File discovery ---\n",
    "EXTS = {\".txt\", \".md\", \".text\", \".pdf\", \".json\", \".csv\", \".html\", \".htm\"}\n",
    "\n",
    "def find_files(root: Path) -> List[Path]:\n",
    "    files = []\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in EXTS:\n",
    "            files.append(p)\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "# --- Loaders by type ---\n",
    "def load_text_plain(fp: Path) -> str:\n",
    "    try:\n",
    "        return fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        return fp.read_text(errors=\"ignore\")\n",
    "\n",
    "def load_text_pdf(fp: Path) -> str:\n",
    "    if PdfReader is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        out = []\n",
    "        reader = PdfReader(str(fp))\n",
    "        for page in reader.pages:\n",
    "            out.append(page.extract_text() or \"\")\n",
    "        return \"\\n\".join(out)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def load_text_json(fp: Path) -> str:\n",
    "    try:\n",
    "        obj = json.loads(fp.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "        # Flatten string-like leaf values\n",
    "        def walk(x):\n",
    "            if isinstance(x, dict):\n",
    "                return \" \".join(walk(v) for v in x.values())\n",
    "            if isinstance(x, list):\n",
    "                return \" \".join(walk(v) for v in x)\n",
    "            if isinstance(x, (str, int, float, bool)):\n",
    "                return str(x)\n",
    "            return \"\"\n",
    "        return walk(obj)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def load_text_csv(fp: Path) -> str:\n",
    "    try:\n",
    "        df = pd.read_csv(fp, nrows=10000)  # cap large files\n",
    "        return \" \".join(map(str, df.astype(str).values.ravel().tolist()))\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = pd.read_table(fp, nrows=10000)\n",
    "            return \" \".join(map(str, df.astype(str).values.ravel().tolist()))\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "def load_text_html(fp: Path) -> str:\n",
    "    if BeautifulSoup is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        html = fp.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        return soup.get_text(\" \", strip=True)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "LOADERS = {\n",
    "    \".txt\":  load_text_plain,\n",
    "    \".md\":   load_text_plain,\n",
    "    \".text\": load_text_plain,\n",
    "    \".pdf\":  load_text_pdf,\n",
    "    \".json\": load_text_json,\n",
    "    \".csv\":  load_text_csv,\n",
    "    \".html\": load_text_html,\n",
    "    \".htm\":  load_text_html,\n",
    "}\n",
    "\n",
    "def normalize_ws(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def chunk_text(s: str, max_tokens: int = 180, overlap: int = 30) -> List[str]:\n",
    "    toks = s.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(toks):\n",
    "        j = min(i + max_tokens, len(toks))\n",
    "        chunk = \" \".join(toks[i:j]).strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        if j == len(toks):\n",
    "            break\n",
    "        i = max(0, j - overlap)\n",
    "    return chunks\n",
    "\n",
    "# --- Artifact I/O ---\n",
    "ART_META   = VDB_DIR / \"chunks_meta.parquet\"\n",
    "ART_CHUNKS = VDB_DIR / \"chunks_text.pkl\"\n",
    "ART_FAISS  = VDB_DIR / \"faiss.index\"\n",
    "ART_GRAPH  = VDB_DIR / \"graph.pkl\"\n",
    "\n",
    "import pickle\n",
    "def save_chunks_text(chunks: List[str]):\n",
    "    with open(ART_CHUNKS, \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "\n",
    "def load_chunks_text() -> List[str]:\n",
    "    with open(ART_CHUNKS, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "print(\"Config loaded.\")\n",
    "print(\"CORPUS_ROOT =\", CORPUS_ROOT)\n",
    "print(\"Vector DB path =\", VDB_DIR)\n",
    "print(\"Extensions searched:\", sorted(EXTS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "433bbcc6-c899-452f-934a-5706a183c486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading+Chunking:  71%|███████████████████████████████▊             | 12/17 [00:59<00:12,  2.51s/it]EOF marker not found\n",
      "Loading+Chunking: 100%|█████████████████████████████████████████████| 17/17 [01:01<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 17 | Chunks: 4381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57fbc2756f941ca9ade8d365a847f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: (4381, 384) elapsed_sec: 4.43\n",
      "Saved:\n",
      " - /home/manny-buff/projects/capstone/week6-rag-graph/artifacts/vdb/chunks_meta.parquet\n",
      " - /home/manny-buff/projects/capstone/week6-rag-graph/artifacts/vdb/chunks_text.pkl\n",
      " - /home/manny-buff/projects/capstone/week6-rag-graph/artifacts/vdb/faiss.index\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell D (guarded): Build Embeddings + FAISS index'\n",
    "- Reads/discovers/loads multiple file types.\n",
    "- Chunks and embeds with e5-small-v2.\n",
    "- Builds FAISS IP index (cosine on normalized vectors).\n",
    "- Guards against empty corpus (prints message and returns early).\n",
    "\"\"\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np, time, faiss\n",
    "\n",
    "files = find_files(CORPUS_ROOT)\n",
    "records = []\n",
    "chunks_text = []\n",
    "\n",
    "for doc_id, fp in enumerate(tqdm(files, desc=\"Loading+Chunking\")):\n",
    "    loader = LOADERS.get(fp.suffix.lower(), load_text_plain)\n",
    "    raw = loader(fp)\n",
    "    text = normalize_ws(raw)\n",
    "    if not text:\n",
    "        continue\n",
    "    parts = chunk_text(text, max_tokens=180, overlap=30)\n",
    "    for k, ch in enumerate(parts):\n",
    "        records.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_id\": len(chunks_text),\n",
    "            \"path\": str(fp),\n",
    "            \"chunk_idx\": k\n",
    "        })\n",
    "        chunks_text.append(ch)\n",
    "\n",
    "import pandas as pd\n",
    "meta_df = pd.DataFrame(records)\n",
    "print(f\"Docs: {len(files)} | Chunks: {len(chunks_text)}\")\n",
    "\n",
    "# Guard: no chunks → stop gracefully\n",
    "if len(chunks_text) == 0:\n",
    "    print(\"No chunks found. Please confirm corpus file types and that loaders extracted text.\")\n",
    "    # Tip for debugging: run the shell probe to see extensions/counts.\n",
    "    raise SystemExit\n",
    "\n",
    "# Embed\n",
    "model = SentenceTransformer(EMBED_ID)\n",
    "t0 = time.time()\n",
    "emb = model.encode(\n",
    "    chunks_text,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "print(\"Embeddings:\", emb.shape, \"elapsed_sec:\", round(time.time()-t0, 2))\n",
    "\n",
    "# Build FAISS\n",
    "dim = emb.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(emb.astype(np.float32))\n",
    "\n",
    "# Save artifacts\n",
    "meta_df.to_parquet(ART_META, index=False)\n",
    "save_chunks_text(chunks_text)\n",
    "faiss.write_index(index, str(ART_FAISS))\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", ART_META)\n",
    "print(\" -\", ART_CHUNKS)\n",
    "print(\" -\", ART_FAISS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939e1843-43da-48ca-bad9-62016dbf1d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built.\n",
      "Nodes: 4381 Edges: 46574\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell E: Build Similarity Graph'\n",
    "Purpose:\n",
    "- Create a lightweight graph of chunk relationships using top-N cosine neighbors.\n",
    "- Collapses edges to doc-level (optional) or keeps chunk-level. We'll keep chunk-level for precision.\n",
    "- Save graph.pkl for later Multi-Hop traversal.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np, networkx as nx, faiss, math\n",
    "\n",
    "# Params for graph density\n",
    "TOP_NEIGHBORS = max(10, RETRIEVER_K * 3)  # small multiple of retriever_k\n",
    "\n",
    "# Load index and chunks\n",
    "index = faiss.read_index(str(ART_FAISS))\n",
    "chunks = load_chunks_text()\n",
    "meta  = pd.read_parquet(ART_META)\n",
    "\n",
    "# Query each vector against index to get neighbors (excluding self)\n",
    "D, I = index.search(emb.astype(np.float32), TOP_NEIGHBORS + 1)\n",
    "\n",
    "G = nx.Graph()\n",
    "for row_idx, nbrs in enumerate(I):\n",
    "    src = int(row_idx)\n",
    "    for rank, nb in enumerate(nbrs):\n",
    "        if nb == -1 or nb == src: \n",
    "            continue\n",
    "        w = float(D[row_idx, rank])\n",
    "        if w <= 0: \n",
    "            continue\n",
    "        # Add undirected edge with weight=max(existing,w)\n",
    "        if G.has_edge(src, nb):\n",
    "            if w > G[src][nb].get(\"weight\", 0.0):\n",
    "                G[src][nb][\"weight\"] = w\n",
    "        else:\n",
    "            G.add_edge(src, nb, weight=w)\n",
    "\n",
    "# Persist graph\n",
    "with open(ART_GRAPH, \"wb\") as f:\n",
    "    pickle.dump(G, f)\n",
    "\n",
    "print(\"Graph built.\")\n",
    "print(\"Nodes:\", G.number_of_nodes(), \"Edges:\", G.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb4d72e-a933-48fb-aadf-b356e294ac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k retrieved:\n",
      "[1] score=0.822 | /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#1033 :: For inspecting a car's fi:ont and rear lights, p. 29, \"Night moves\" > Minor tUe for mounting on the ceil- ing of a closet to see what is …\n",
      "[2] score=0.815 | /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#1008 :: for a child's dresser, p. 21, \"Playful pulls\" Aluminum foil >■ Wrapped around pillows, to keep cats off the sofa, p. 41, \"Stay off the …\n",
      "[3] score=0.815 | /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#1030 :: pour before completion, p. 258, \"Easy pour\" >■ To prevent a toilet bowl from \"sweating\" m humid weather, p. 173, \"Bathroom condensation\" …\n",
      "[4] score=0.815 | /home/manny-buff/projects/capstone/hw-rag/data/the-complete-idiots-guide-to-simple-home-repair.pdf | chunk#26 :: motion. Warmest thanks are also due to Lynn Northrup, Jan Lynn, and Ross Patty, whose thoughtful suggestions and unflagging attention made …\n",
      "[5] score=0.812 | /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#1036 :: \"Do the pan-pan\" Pants hanger, wood >• To store paper bags, p. 33, \"How to brown bag it?\" >• Filled with manure and j water, for …\n",
      "\n",
      "Graph neighbors (first 10) of top chunk: 1033\n",
      "Neighbors: [1012, 1019, np.int64(1051), np.int64(1042), np.int64(1034), np.int64(1043), np.int64(1032), np.int64(1023), np.int64(1048), np.int64(1018)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell F: Validate'\n",
    "Purpose:\n",
    "- Issue a sample query to FAISS, print top-k chunk previews\n",
    "- Show 1-step neighbors in the graph for the top hit (sanity for Multi-Hop)\n",
    "\"\"\"\n",
    "\n",
    "import textwrap, faiss, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "index = faiss.read_index(str(ART_FAISS))\n",
    "meta  = pd.read_parquet(ART_META)\n",
    "chunks = load_chunks_text()\n",
    "enc   = SentenceTransformer(EMBED_ID)\n",
    "\n",
    "query = \"Briefly summarize the core topic of this corpus.\"\n",
    "qv = enc.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "D, I = index.search(qv.astype(np.float32), RETRIEVER_K)\n",
    "\n",
    "print(\"Top-k retrieved:\")\n",
    "for rank, cid in enumerate(I[0]):\n",
    "    doc = meta.loc[meta[\"chunk_id\"]==cid].iloc[0]\n",
    "    preview = textwrap.shorten(chunks[cid], width=140, placeholder=\" …\")\n",
    "    print(f\"[{rank+1}] score={D[0,rank]:.3f} | {doc['path']} | chunk#{doc['chunk_idx']} :: {preview}\")\n",
    "\n",
    "# Graph neighbor preview for top hit\n",
    "top_chunk = int(I[0,0])\n",
    "print(\"\\nGraph neighbors (first 10) of top chunk:\", top_chunk)\n",
    "with open(ART_GRAPH, \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "nbrs = list(G.neighbors(top_chunk))[:10]\n",
    "print(\"Neighbors:\", nbrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef0811e-362f-4e56-b3ab-1351b356ea44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded meta: (4381, 4) | chunks: 4381\n",
      "FAISS dims: 384 | Graph nodes/edges: 4381 46574\n",
      "LLM local path exists: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell G: Load config + artifacts + embedder'\n",
    "Purpose:\n",
    "- Read run config (same JSON as Week6-1).\n",
    "- Load FAISS, graph, metadata, and chunks.\n",
    "- Initialize e5-small-v2 embedder.\n",
    "\"\"\"\n",
    "\n",
    "import json, pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss, networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 'Paths'\n",
    "ROOT = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph\")\n",
    "CFG_RUN = ROOT / \"configs\" / \"rag_graph_run_config.json\"\n",
    "VDB     = ROOT / \"artifacts\" / \"vdb\"\n",
    "\n",
    "# 'Artifacts'\n",
    "ART_META   = VDB / \"chunks_meta.parquet\"\n",
    "ART_CHUNKS = VDB / \"chunks_text.pkl\"\n",
    "ART_FAISS  = VDB / \"faiss.index\"\n",
    "ART_GRAPH  = VDB / \"graph.pkl\"\n",
    "\n",
    "# 'Load config'\n",
    "cfg = json.loads(CFG_RUN.read_text())\n",
    "CORPUS_ROOT  = Path(cfg[\"corpus_root\"])\n",
    "EMBED_ID     = cfg[\"embed_model\"]              # e.g., 'intfloat/e5-small-v2'\n",
    "LLM_MODEL_ID = cfg[\"llm_model_id\"]             # e.g., 'Qwen/Qwen2.5-VL-3B-Instruct'\n",
    "LLM_LOCAL    = Path(cfg[\"llm_local_path\"])     # local Qwen path\n",
    "RETRIEVER_K  = int(cfg.get(\"retriever_k\", 5))\n",
    "HOP_LIMIT    = int(cfg.get(\"hop_limit\", 2))\n",
    "\n",
    "# 'Load artifacts'\n",
    "meta   = pd.read_parquet(ART_META)\n",
    "chunks = pickle.loads(ART_CHUNKS.read_bytes())\n",
    "index  = faiss.read_index(str(ART_FAISS))\n",
    "with open(ART_GRAPH, \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "# 'Embedder'\n",
    "embedder = SentenceTransformer(EMBED_ID)\n",
    "\n",
    "print(\"Loaded meta:\", meta.shape, \"| chunks:\", len(chunks))\n",
    "print(\"FAISS dims:\", index.d, \"| Graph nodes/edges:\", G.number_of_nodes(), G.number_of_edges())\n",
    "print(\"LLM local path exists:\", LLM_LOCAL.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9451faf6-f236-434a-98aa-c0284529fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "'Cell H: Retrieval + Graph Expansion + Context Builder'\n",
    "Purpose:\n",
    "- Provide dense retriever (FAISS).\n",
    "- Provide neighbor expansion up to HOP_LIMIT with breadth cap.\n",
    "- Consolidate chunks into a prompt context (size-bounded).\n",
    "\"\"\"\n",
    "\n",
    "import math, textwrap\n",
    "from typing import List, Set, Dict\n",
    "\n",
    "def dense_retrieve(query: str, top_k: int) -> List[int]:\n",
    "    # 'Encode query and search FAISS; return chunk IDs'\n",
    "    q = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    D, I = index.search(q.astype(np.float32), top_k)\n",
    "    return [int(x) for x in I[0]]\n",
    "\n",
    "def expand_via_graph(seed_ids: List[int], hop_limit: int, per_seed_cap: int = 20, global_cap: int = 200) -> List[int]:\n",
    "    # 'Expand neighbors up to hop_limit; cap breadth; dedupe; return chunk IDs'\n",
    "    visited: Set[int] = set(int(s) for s in seed_ids)\n",
    "    frontier: Set[int] = set(visited)\n",
    "    for hop in range(hop_limit):\n",
    "        next_frontier: Set[int] = set()\n",
    "        for node in list(frontier):\n",
    "            nbrs = list(G.neighbors(node))\n",
    "            # limit per-seed to avoid explosion\n",
    "            for nb in nbrs[:per_seed_cap]:\n",
    "                nb = int(nb)  # guard np.int64\n",
    "                if nb not in visited:\n",
    "                    next_frontier.add(nb)\n",
    "        frontier = next_frontier\n",
    "        visited.update(frontier)\n",
    "        if len(visited) >= global_cap:\n",
    "            break\n",
    "    return list(visited)\n",
    "\n",
    "def build_context(chunk_ids: List[int], max_chars: int = 4000) -> str:\n",
    "    # 'Join snippets with lightweight headers; stop at char budget'\n",
    "    out = []\n",
    "    size = 0\n",
    "    for cid in chunk_ids:\n",
    "        row = meta.loc[meta[\"chunk_id\"] == cid]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        path = row.iloc[0][\"path\"]\n",
    "        idx  = row.iloc[0][\"chunk_idx\"]\n",
    "        snippet = textwrap.shorten(chunks[cid], width=360, placeholder=\" …\")\n",
    "        block = f\"[SOURCE] {path} | chunk#{idx}\\n{snippet}\\n\"\n",
    "        if size + len(block) > max_chars:\n",
    "            break\n",
    "        out.append(block)\n",
    "        size += len(block)\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def retrieve_expand_context(query: str, top_k: int = None, hop_limit: int = None, per_seed_cap: int = 20, global_cap: int = 200, max_chars: int = 4000):\n",
    "    # 'End-to-end helper: dense retrieval → graph expand → context'\n",
    "    if top_k is None:    top_k    = RETRIEVER_K\n",
    "    if hop_limit is None: hop_limit = HOP_LIMIT\n",
    "\n",
    "    seeds = dense_retrieve(query, top_k)\n",
    "    expanded = expand_via_graph(seeds, hop_limit, per_seed_cap=per_seed_cap, global_cap=global_cap)\n",
    "    # order: seeds first, then expanded (stable, unique)\n",
    "    ordered = []\n",
    "    seen = set()\n",
    "    for x in seeds + expanded:\n",
    "        if x not in seen:\n",
    "            ordered.append(x)\n",
    "            seen.add(x)\n",
    "    context = build_context(ordered, max_chars=max_chars)\n",
    "    return seeds, ordered, context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ada504c1-b924-456e-9e51-3474f06ca155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QwenAnswerer (VL-aware, warning-free) ready. Call qwen.load() before answering.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell I: Qwen Answer Synthesis (VL-aware, warning-free)'\n",
    "- Uses AutoProcessor(use_fast=False) to silence the fast/slow warning.\n",
    "- Uses AutoModelForImageTextToText (replaces deprecated AutoModelForVision2Seq).\n",
    "- Uses chat template when available for better instruction adherence.\n",
    "- Deterministic generation: no temperature/top_p/top_k; do_sample=False.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoConfig, AutoProcessor, AutoModelForImageTextToText,\n",
    "    AutoTokenizer, AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "class QwenAnswerer:\n",
    "    def __init__(self, local_dir: Path, model_id: str, device: str = \"cuda\"):\n",
    "        self.local_dir = local_dir\n",
    "        self.model_id  = model_id\n",
    "        self.device    = device\n",
    "        self.is_vl     = False\n",
    "        self.processor = None\n",
    "        self.tokenizer = None\n",
    "        self.model     = None\n",
    "\n",
    "    def _load_from(self, src: str):\n",
    "        cfg = AutoConfig.from_pretrained(src, trust_remote_code=True)\n",
    "        mtype = getattr(cfg, \"model_type\", \"\").lower()\n",
    "\n",
    "        if \"vl\" in mtype:  # qwen2_vl / qwen2_5_vl\n",
    "            self.is_vl = True\n",
    "            # use_fast=False: silences the processor warning and preserves old behavior\n",
    "            self.processor = AutoProcessor.from_pretrained(src, trust_remote_code=True, use_fast=False)\n",
    "            self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "                src, trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            self.is_vl = False\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(src, trust_remote_code=True)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                src, trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "\n",
    "    def load(self):\n",
    "        last_err = None\n",
    "        sources = []\n",
    "        if self.local_dir.exists():\n",
    "            sources.append(str(self.local_dir))\n",
    "        sources.append(self.model_id)\n",
    "        for src in sources:\n",
    "            try:\n",
    "                self._load_from(src)\n",
    "                return\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                self.processor = self.tokenizer = self.model = None\n",
    "        raise RuntimeError(f\"Failed to load Qwen (VL-aware). Last error: {last_err}\")\n",
    "\n",
    "    def _format_prompt(self, question: str, context: str) -> dict:\n",
    "        \"\"\"\n",
    "        Returns a dict with tokenization-ready inputs.\n",
    "        Uses chat template if available for better instruction following.\n",
    "        \"\"\"\n",
    "        system_msg = \"You are a concise home-repair RAG assistant. Use ONLY the provided context. If missing info, say so.\"\n",
    "        user_msg   = f\"Question:\\n{question}\\n\\nContext:\\n{context}\\n\\nRespond concisely and cite key source file names.\"\n",
    "\n",
    "        if self.is_vl:\n",
    "            tok = self.processor.tokenizer\n",
    "        else:\n",
    "            tok = self.tokenizer\n",
    "\n",
    "        apply_chat = getattr(tok, \"apply_chat_template\", None)\n",
    "        if callable(apply_chat):\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_msg},\n",
    "                {\"role\": \"user\",   \"content\": user_msg}\n",
    "            ]\n",
    "            prompt_text = apply_chat(messages, tokenize=False, add_generation_prompt=True)\n",
    "        else:\n",
    "            prompt_text = f\"{system_msg}\\n\\n{user_msg}\"\n",
    "\n",
    "        return {\"text\": prompt_text}\n",
    "\n",
    "    def answer(self, question: str, context: str, max_new_tokens: int = 240) -> str:\n",
    "        assert self.model is not None, \"Model not loaded\"\n",
    "        prompt_dict = self._format_prompt(question, context)\n",
    "\n",
    "        if self.is_vl:\n",
    "            # text-only path via processor; deterministic generation (no sampling flags)\n",
    "            inputs = self.processor(**prompt_dict, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                out = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            text = self.processor.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        else:\n",
    "            inputs = self.tokenizer(prompt_dict[\"text\"], return_tensors=\"pt\").to(self.model.device)\n",
    "            with torch.no_grad():\n",
    "                out = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            text = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "qwen = QwenAnswerer(LLM_LOCAL, LLM_MODEL_ID)\n",
    "print(\"QwenAnswerer (VL-aware, warning-free) ready. Call qwen.load() before answering.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "127fd223-1d82-4670-9d50-a7c06f0ec8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds: [1203, 528, 287, 507, 1199] … total: 5\n",
      "Expanded set size: 813\n",
      "Context preview:\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/A Dirty Guide to a Clean Home _ Housekeeping Hacks You Cant Live Without.pdf | chunk#107\n",
      "dusty and are a pain to clean. There is no need for a fancy tool to clean them, your hand is the perfect solution. The easiest way is just to grab a sock, slightly dampen it, put it over your hand, grasp each slat, and glide it along from one side to the other. If your blinds need a super deep clean, you can give them a bath. Yes, that’s right. If the …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.p…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079acaa9ba454018a48f4efb3b08ed06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Answer ===\n",
      " system\n",
      "You are a concise home-repair RAG assistant. Use ONLY the provided context. If missing info, say so.\n",
      "user\n",
      "Question:\n",
      "How can I stop a toilet tank from sweating in humid weather, and what simple materials do I need?\n",
      "\n",
      "Context:\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/A Dirty Guide to a Clean Home _ Housekeeping Hacks You Cant Live Without.pdf | chunk#107\n",
      "dusty and are a pain to clean. There is no need for a fancy tool to clean them, your hand is the perfect solution. The easiest way is just to grab a sock, slightly dampen it, put it over your hand, grasp each slat, and glide it along from one side to the other. If your blinds need a super deep clean, you can give them a bath. Yes, that’s right. If the …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#528\n",
      "Royal flush. Try replacing an old toilet writh a new low- flow model. A family of four can save over 20,000 gallons of water each year. Ask your plumber about the models that work best. No goldbricldng. Don t put a brick in your toilet tank to save water by taking up space; it wrtll eventually disintegrate, causing the toilet to leak. Instead, use a water- …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#287\n",
      "absorbing it, put a layer of clay beneath the topsoil or sod. Cracks are big trouble. Open cracks along the base- ment floor, or vertical cracks along a basement wall that are wider at the top than at the bottom, indicate a possible problem with the footings beneath the foun- dation. Consult a structural engmeer for advice Clear drainpipes. Roof drainpipes …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#507\n",
      "low speed. For a laminate countertop. use a hole saw. With an adjustable wrench, mount the faucet according to the directions. Be careful: the sink hole will be very sharp. 2 Choose a convenient place for the filter system: in the sink cabinet, for instance, or in the basement below. Mount the unit on a solid surface with screws through mounting holes, or …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/A Dirty Guide to a Clean Home _ Housekeeping Hacks You Cant Live Without.pdf | chunk#103\n",
      "the ceiling? That’s probably an exhaust fan. They are an important part of the home’s ventilation system. They suck up moisture, humidity that can lead to mold and mildew growth, and eliminate odor. They exist almost exclusively in bathrooms, which get steamy, but if you live in an apartment building, you might also have one in your kitchen. To clean any …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/Complete home repair  with 350 projects and 2300 photos.pdf | chunk#577\n",
      "on each side to allow for expansion. Attach the sheathing to the nailing strips and studs, using 2Y4\" deck screws driven every 12\" (photo C). Tools: Hammer, circular saw, tape measure, chalk line, pry bar, drill. Materials: sheathing, 2x4 lumber, 3\" deck screws, 21/,\" deck screws. Leave a 14\" expansion gap around the new sheathing and install with 214\" …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/Complete home repair  with 350 projects and 2300 photos.pdf | chunk#578\n",
      "jointing tool. Materials: Mortar mix, concrete for¬ tifier, mortar pigment. determine whether they’re sound. Tuckpoint deteriorated joints. Spalling occurs when trapped moisture is exposed to repeated freeze and thaw cycles, exerting enough directional pressure to fracture Tuckpoint joints by removing cracked, damaged mortar and filling the joints with …\n",
      "\n",
      "[SOURCE] /home/manny-buff/projects/capstone/hw-rag/data/1001 do-it-yourself hints & tips  tricks.pdf | chunk#14\n",
      "the Off position to shut off the power On a cartridge- Pull out the plastic boxes holding type box the cartridge fuses. On the main circuit Flip the one or two main switches breaker box to Off. On other circuit breaker boxes NATURAL GAS Flip all switches to Off CAtJTION: If you smell gas, open the windows and shut off the main gas valve. Do not light a …\n",
      "\n",
      "\n",
      "Respond concisely and cite key source file names.\n",
      "assistant\n",
      "To stop a toilet tank from sweating in humid weather, you can use a damp sock to slide along the slats. You can also use a layer of clay beneath the topsoil or sod to absorb moisture. To clean any cracks in the basement floor or wall, consult a structural engineer. To clean a laminate countertop, use a hole saw and attach the sheathing to the nailing strips and studs with 2x4 lumber and 3\" deck screws. To clean exhaust fans, install a filter system in the sink cabinet or basement below. \n",
      "\n",
      "Latency (s): 1.05\n",
      "Ablation row appended to /home/manny-buff/projects/capstone/week6-rag-graph/artifacts/ablation_results_graph.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell J: Run query + log ablation row'\n",
    "Purpose:\n",
    "- Execute retrieve → expand → context build.\n",
    "- Load Qwen and generate answer.\n",
    "- Append an ablation row to CSV.\n",
    "\"\"\"\n",
    "\n",
    "import time, csv, os\n",
    "ABL_PATH = ROOT / \"artifacts\" / \"ablation_results_graph.csv\"\n",
    "\n",
    "query = \"How can I stop a toilet tank from sweating in humid weather, and what simple materials do I need?\"\n",
    "seeds, expanded, context = retrieve_expand_context(query, top_k=RETRIEVER_K, hop_limit=HOP_LIMIT, per_seed_cap=20, global_cap=200, max_chars=4000)\n",
    "\n",
    "print(\"Seeds:\", seeds[:10], \"… total:\", len(seeds))\n",
    "print(\"Expanded set size:\", len(expanded))\n",
    "print(\"Context preview:\\n\", context[:600], \"…\", sep=\"\")\n",
    "\n",
    "\"\"\"\n",
    "Small patch: keep Cell J logic as-is, but no sampling flags are passed by QwenAnswerer now.\n",
    "Just re-run this cell after re-running Cell I and calling qwen.load().\n",
    "\"\"\"\n",
    "\n",
    "# (re-run as before)\n",
    "qwen.load()\n",
    "t0 = time.time()\n",
    "answer = qwen.answer(query, context, max_new_tokens=256)\n",
    "elapsed = round(time.time() - t0, 2)\n",
    "\n",
    "print(\"\\n=== Answer ===\\n\", answer, \"\\n\")\n",
    "print(\"Latency (s):\", elapsed)\n",
    "# ablation append stays unchanged\n",
    "\n",
    "# 'Log ablation row (accuracy left blank for manual scoring later)'\n",
    "row = {\n",
    "    \"variant\": \"dense+graph\",\n",
    "    \"retriever_k\": RETRIEVER_K,\n",
    "    \"hop_limit\": HOP_LIMIT,\n",
    "    \"accuracy\": \"\",\n",
    "    \"notes\": f\"seeds={len(seeds)} expanded={len(expanded)} latency_s={elapsed}\"\n",
    "}\n",
    "\n",
    "# Append row\n",
    "header = [\"variant\", \"retriever_k\", \"hop_limit\", \"accuracy\", \"notes\"]\n",
    "file_exists = ABL_PATH.exists()\n",
    "with open(ABL_PATH, \"a\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=header)\n",
    "    if not file_exists:\n",
    "        w.writeheader()\n",
    "    w.writerow(row)\n",
    "\n",
    "print(f\"Ablation row appended to {ABL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83257a49-4cc3-459c-9e31-5ab292c6a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 6 — Part 2: Multi-Hop QA (Summary)\n",
    "'''\n",
    "**Retriever**: FAISS (cosine on normalized e5-small-v2)  \n",
    "**Graph Expansion**: neighbor hops = `HOP_LIMIT` (default 2), per-seed cap = 20, global cap = 200  \n",
    "**LLM**: Qwen-VL (VL-aware loader, deterministic `do_sample=False`)  \n",
    "**Prompting**: Chat template when available; system+user roles; context-only constraint\n",
    "\n",
    "### Pipeline\n",
    "1. **Dense Retrieval** → top-K chunk IDs.\n",
    "2. **Graph Expansion** → add neighbors (bounded breadth), dedupe, seeds prioritized.\n",
    "3. **Context Build** → path+chunk headers with short previews, size-bounded.\n",
    "4. **Answer Synthesis** → Qwen-VL text-only path; concise answer with file citations.\n",
    "\n",
    "### Validations\n",
    "- Seeds/Expanded sizes printed.\n",
    "- Answer generated without processor/deprecation/generation warnings.\n",
    "- Ablation row appended to `artifacts/ablation_results_graph.csv`.\n",
    "\n",
    "**Determinism**: No sampling; repeated runs on the same artifacts/config should match.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deea3254-25e4-43ea-8006-1974e191fb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /home/manny-buff/projects/capstone/week6-rag-graph/artifacts/Report_snippets_Wk6_1_2.md\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Report Retrieval Script #1 (Week6-1 & Week6-2)'\n",
    "Purpose:\n",
    "- Create or update a single markdown file summarizing environment, config,\n",
    "  index/graph stats, and ablation results for Parts 1 & 2.\n",
    "- Output: artifacts/Report_snippets_Wk6_1_2.md\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import json, pickle, pandas as pd\n",
    "\n",
    "ROOT = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph\")\n",
    "OUT  = ROOT / \"artifacts\" / \"Report_snippets_Wk6_1_2.md\"\n",
    "\n",
    "CFG_ENV = ROOT / \"configs\" / \"env_rag_graph.json\"\n",
    "CFG_RUN = ROOT / \"configs\" / \"rag_graph_run_config.json\"\n",
    "VDB     = ROOT / \"artifacts\" / \"vdb\"\n",
    "\n",
    "ART_META   = VDB / \"chunks_meta.parquet\"\n",
    "ART_FAISS  = VDB / \"faiss.index\"\n",
    "ART_GRAPH  = VDB / \"graph.pkl\"\n",
    "ABL_CSV    = ROOT / \"artifacts\" / \"ablation_results_graph.csv\"\n",
    "\n",
    "# Load pieces (best-effort)\n",
    "env_info = json.loads(CFG_ENV.read_text()) if CFG_ENV.exists() else {}\n",
    "run_cfg  = json.loads(CFG_RUN.read_text()) if CFG_RUN.exists() else {}\n",
    "meta_df  = pd.read_parquet(ART_META) if ART_META.exists() else pd.DataFrame()\n",
    "graph_n  = graph_e = None\n",
    "if ART_GRAPH.exists():\n",
    "    with open(ART_GRAPH, \"rb\") as f:\n",
    "        G = pickle.load(f)\n",
    "        graph_n, graph_e = G.number_of_nodes(), G.number_of_edges()\n",
    "abl_df = pd.read_csv(ABL_CSV) if ABL_CSV.exists() else pd.DataFrame()\n",
    "\n",
    "# Compose markdown\n",
    "lines = []\n",
    "lines.append(\"# Week 6 — Report Snippets (Parts 1 & 2)\\n\")\n",
    "lines.append(\"## Environment\")\n",
    "lines.append(f\"- Python venv: `{env_info.get('python_venv','')}`\")\n",
    "lines.append(f\"- Python: `{env_info.get('python_version','')}`\")\n",
    "lines.append(f\"- GPU: `{env_info.get('nvidia_smi','')}`\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Run Config\")\n",
    "for k in [\"corpus_root\",\"embed_model\",\"llm_model_id\",\"llm_local_path\",\"device\",\"retriever_k\",\"hop_limit\"]:\n",
    "    lines.append(f\"- {k}: `{run_cfg.get(k,'')}`\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Artifacts\")\n",
    "lines.append(f\"- Meta rows (chunks): {len(meta_df) if not meta_df.empty else 0}\")\n",
    "lines.append(f\"- FAISS index: {'present' if ART_FAISS.exists() else 'missing'}\")\n",
    "if graph_n is not None:\n",
    "    lines.append(f\"- Graph nodes: {graph_n} | edges: {graph_e}\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Ablation Results (head)\")\n",
    "if not abl_df.empty:\n",
    "    lines.append(abl_df.head(5).to_markdown(index=False))\n",
    "else:\n",
    "    lines.append(\"_No ablation rows yet._\")\n",
    "lines.append(\"\")\n",
    "\n",
    "lines.append(\"## Ablation Results (tail)\")\n",
    "if not abl_df.empty:\n",
    "    lines.append(abl_df.tail(5).to_markdown(index=False))\n",
    "else:\n",
    "    lines.append(\"_No ablation rows yet._\")\n",
    "\n",
    "OUT.write_text(\"\\n\".join(lines))\n",
    "print(f\"Wrote: {OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5447bfcb-2db5-4ad4-add1-9e32ecb2c8b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Deprecated cell — see 'Execution Notes' above. Use Cell K + terminal launch instead.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m Deprecated cell — see 'Execution Notes' above. Use Cell K + terminal launch instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manny-buff/venvs/core-rag/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Deprecated / Skip\n",
    "raise SystemExit(\"Deprecated cell — see 'Execution Notes' above. Use Cell K + terminal launch instead.\")\n",
    "\n",
    "\"\"\"\n",
    "'Cell K: Write Streamlit App'\n",
    "Purpose:\n",
    "- Generate a minimal Streamlit application that uses the existing Week6-1/2 artifacts:\n",
    "  dense retrieval -> graph expansion -> MMR re-ranking -> context -> Qwen answer (deterministic).\n",
    "- The app logs each query/answer to CSV under artifacts/app_logs.csv for later reporting.\n",
    "\n",
    "Notes:\n",
    "- No external .py packaging needed; this cell writes a single file and we invoke Streamlit on it.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "APP_DIR = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph/app\")\n",
    "APP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "APP_PATH = APP_DIR / \"app_rag.py\"\n",
    "\n",
    "app_code = r'''# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Streamlit UI for Qwen-Graph-RAG\n",
    "- Loads Week6 run_config and artifacts\n",
    "- Dense retrieval -> Graph expansion -> MMR re-ranking -> Context build -> Deterministic Qwen answer\n",
    "- Logs to artifacts/app_logs.csv\n",
    "\"\"\"\n",
    "import os, json, time, csv, math, textwrap, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "\n",
    "# --- Config & Artifacts ---\n",
    "ROOT = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph\")\n",
    "CFG_RUN = ROOT / \"configs\" / \"rag_graph_run_config.json\"\n",
    "VDB     = ROOT / \"artifacts\" / \"vdb\"\n",
    "ART_META   = VDB / \"chunks_meta.parquet\"\n",
    "ART_CHUNKS = VDB / \"chunks_text.pkl\"\n",
    "ART_FAISS  = VDB / \"faiss.index\"\n",
    "ART_GRAPH  = VDB / \"graph.pkl\"\n",
    "LOG_CSV    = ROOT / \"artifacts\" / \"app_logs.csv\"\n",
    "\n",
    "# --- Caches ---\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_config():\n",
    "    cfg = json.loads(CFG_RUN.read_text())\n",
    "    return cfg\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_artifacts():\n",
    "    import faiss, networkx as nx\n",
    "    meta   = pd.read_parquet(ART_META)\n",
    "    chunks = pickle.loads(ART_CHUNKS.read_bytes())\n",
    "    index  = faiss.read_index(str(ART_FAISS))\n",
    "    with open(ART_GRAPH, \"rb\") as f:\n",
    "        G = pickle.load(f)\n",
    "    return meta, chunks, index, G\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def get_embedder(model_id: str):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(model_id)\n",
    "    return model\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def get_qwen(local_dir: Path, model_id: str):\n",
    "    import torch\n",
    "    from transformers import (\n",
    "        AutoConfig, AutoProcessor, AutoModelForImageTextToText,\n",
    "        AutoTokenizer, AutoModelForCausalLM\n",
    "    )\n",
    "    class QwenAnswerer:\n",
    "        def __init__(self, local_dir: Path, model_id: str):\n",
    "            self.local_dir = local_dir\n",
    "            self.model_id  = model_id\n",
    "            self.is_vl     = False\n",
    "            self.processor = None\n",
    "            self.tokenizer = None\n",
    "            self.model     = None\n",
    "\n",
    "        def _load_from(self, src: str):\n",
    "            cfg = AutoConfig.from_pretrained(src, trust_remote_code=True)\n",
    "            mtype = getattr(cfg, \"model_type\", \"\").lower()\n",
    "            if \"vl\" in mtype:\n",
    "                self.is_vl = True\n",
    "                self.processor = AutoProcessor.from_pretrained(src, trust_remote_code=True, use_fast=False)\n",
    "                self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "                    src, trust_remote_code=True,\n",
    "                    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "                    device_map=\"auto\"\n",
    "                )\n",
    "            else:\n",
    "                self.is_vl = False\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(src, trust_remote_code=True)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    src, trust_remote_code=True,\n",
    "                    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "                    device_map=\"auto\"\n",
    "                )\n",
    "\n",
    "        def load(self):\n",
    "            last_err = None\n",
    "            sources = []\n",
    "            if self.local_dir.exists():\n",
    "                sources.append(str(self.local_dir))\n",
    "            sources.append(self.model_id)\n",
    "            for src in sources:\n",
    "                try:\n",
    "                    self._load_from(src)\n",
    "                    return self\n",
    "                except Exception as e:\n",
    "                    last_err = e\n",
    "                    self.processor = self.tokenizer = self.model = None\n",
    "            raise RuntimeError(f\"Failed to load Qwen. Last error: {last_err}\")\n",
    "\n",
    "        def _format_prompt(self, question: str, context: str) -> dict:\n",
    "            system_msg = \"You are a concise home-repair RAG assistant. Use ONLY the provided context. If missing info, say so.\"\n",
    "            user_msg   = f\"Question:\\n{question}\\n\\nContext:\\n{context}\\n\\nRespond concisely and cite key source file names.\"\n",
    "            if self.is_vl:\n",
    "                tok = self.processor.tokenizer\n",
    "            else:\n",
    "                tok = self.tokenizer\n",
    "            apply_chat = getattr(tok, \"apply_chat_template\", None)\n",
    "            if callable(apply_chat):\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_msg},\n",
    "                    {\"role\": \"user\",   \"content\": user_msg}\n",
    "                ]\n",
    "                prompt_text = apply_chat(messages, tokenize=False, add_generation_prompt=True)\n",
    "            else:\n",
    "                prompt_text = f\"{system_msg}\\n\\n{user_msg}\"\n",
    "            return {\"text\": prompt_text}\n",
    "\n",
    "        def answer(self, question: str, context: str, max_new_tokens: int = 240):\n",
    "            assert self.model is not None\n",
    "            prompt = self._format_prompt(question, context)\n",
    "            if hasattr(self, \"processor\") and self.processor is not None and self.is_vl:\n",
    "                inputs = self.processor(**prompt, return_tensors=\"pt\")\n",
    "                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "                with torch.no_grad():\n",
    "                    out = self.model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "                text = self.processor.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "            else:\n",
    "                inputs = self.tokenizer(prompt[\"text\"], return_tensors=\"pt\").to(self.model.device)\n",
    "                with torch.no_grad():\n",
    "                    out = self.model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "                text = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "            return text.strip()\n",
    "\n",
    "    return QwenAnswerer(local_dir, model_id).load()\n",
    "\n",
    "# --- Retrieval utilities ---\n",
    "def dense_retrieve(embedder, index, query: str, top_k: int):\n",
    "    qv = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    D, I = index.search(qv.astype(np.float32), top_k)\n",
    "    return qv[0], [int(x) for x in I[0]], D[0].tolist()\n",
    "\n",
    "def expand_via_graph(G, seeds, hop_limit: int, per_seed_cap: int = 20, global_cap: int = 200):\n",
    "    visited = set(int(s) for s in seeds)\n",
    "    frontier = set(visited)\n",
    "    for _ in range(hop_limit):\n",
    "        nxt = set()\n",
    "        for node in list(frontier):\n",
    "            nbrs = list(G.neighbors(node))\n",
    "            for nb in nbrs[:per_seed_cap]:\n",
    "                nb = int(nb)\n",
    "                if nb not in visited:\n",
    "                    nxt.add(nb)\n",
    "        frontier = nxt\n",
    "        visited.update(frontier)\n",
    "        if len(visited) >= global_cap:\n",
    "            break\n",
    "    return list(visited)\n",
    "\n",
    "def mmr_select(embedder, query_vec, candidate_ids, chunks, k=12, lambda_weight=0.70):\n",
    "    # Compute embeddings for candidates (batch)\n",
    "    ctext = [chunks[cid] for cid in candidate_ids]\n",
    "    C = embedder.encode(ctext, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    # Similarity query->candidate\n",
    "    sim_q = C @ query_vec  # cosine if normalized\n",
    "    selected = []\n",
    "    remaining = list(range(len(candidate_ids)))\n",
    "    # Greedy MMR\n",
    "    while remaining and len(selected) < k:\n",
    "        if not selected:\n",
    "            idx = int(np.argmax(sim_q[remaining]))\n",
    "            chosen = remaining.pop(idx)\n",
    "            selected.append(chosen)\n",
    "            continue\n",
    "        # diversity term: max sim to already selected\n",
    "        S = C[[remaining]] @ C[[candidate_ids.index(candidate_ids[s]) for s in selected]].T  # but we need matching shapes\n",
    "        # Simpler: compute max sim to any selected row directly\n",
    "        max_sim = np.max(C[remaining] @ C[selected].T, axis=1)\n",
    "        # MMR score\n",
    "        mmr = lambda_weight * sim_q[remaining] - (1 - lambda_weight) * max_sim\n",
    "        idx = int(np.argmax(mmr))\n",
    "        chosen = remaining.pop(idx)\n",
    "        selected.append(chosen)\n",
    "    # Map back to chunk ids\n",
    "    return [candidate_ids[i] for i in selected]\n",
    "\n",
    "def build_context(meta, chunks, ordered_ids, max_chars=4000):\n",
    "    out = []\n",
    "    size = 0\n",
    "    for cid in ordered_ids:\n",
    "        row = meta.loc[meta[\"chunk_id\"]==cid]\n",
    "        if row.empty: \n",
    "            continue\n",
    "        path = row.iloc[0][\"path\"]\n",
    "        idx  = row.iloc[0][\"chunk_idx\"]\n",
    "        snippet = textwrap.shorten(chunks[cid], width=360, placeholder=\" …\")\n",
    "        block = f\"[SOURCE] {path} | chunk#{idx}\\n{snippet}\\n\"\n",
    "        if size + len(block) > max_chars:\n",
    "            break\n",
    "        out.append(block)\n",
    "        size += len(block)\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "# --- UI ---\n",
    "st.set_page_config(page_title=\"Qwen Graph-RAG\", page_icon=\"🧭\", layout=\"wide\")\n",
    "st.title(\"Qwen Graph-RAG (Week 6 — Application)\")\n",
    "\n",
    "cfg = load_config()\n",
    "meta, chunks, index, G = load_artifacts()\n",
    "embedder = get_embedder(cfg[\"embed_model\"])\n",
    "qwen = None\n",
    "\n",
    "with st.sidebar:\n",
    "    st.subheader(\"Settings\")\n",
    "    top_k    = st.slider(\"Retriever K\", 3, 20, int(cfg.get(\"retriever_k\", 5)), 1)\n",
    "    hop_lim  = st.slider(\"Hop Limit\", 0, 3, int(cfg.get(\"hop_limit\", 2)), 1)\n",
    "    per_cap  = st.slider(\"Per-seed Neighbor Cap\", 5, 50, 15, 1)\n",
    "    glob_cap = st.slider(\"Global Cap\", 50, 800, 200, 50)\n",
    "    mmr_k    = st.slider(\"MMR Select K (context items)\", 4, 20, 12, 1)\n",
    "    mmr_lmb  = st.slider(\"MMR λ (relevance vs diversity)\", 0.10, 0.95, 0.70, 0.05)\n",
    "    max_chars= st.slider(\"Context Max Chars\", 1000, 8000, 4000, 250)\n",
    "    load_llm = st.checkbox(\"Load Qwen model\", value=False, help=\"Check this once before first query.\")\n",
    "    st.caption(\"Deterministic generation (do_sample=False).\")\n",
    "\n",
    "if load_llm and qwen is None:\n",
    "    try:\n",
    "        qwen = get_qwen(Path(cfg[\"llm_local_path\"]), cfg[\"llm_model_id\"])\n",
    "        st.success(\"Qwen loaded.\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"Qwen load error: {e}\")\n",
    "\n",
    "query = st.text_area(\"Ask a question about home repair:\", height=100, placeholder=\"e.g., How can I stop a toilet tank from sweating in humid weather?\")\n",
    "run = st.button(\"Search & Answer\")\n",
    "\n",
    "colL, colR = st.columns([1,1])\n",
    "\n",
    "if run and query.strip():\n",
    "    t0 = time.time()\n",
    "    qv, seeds, scores = dense_retrieve(embedder, index, query, top_k=top_k)\n",
    "    expanded = expand_via_graph(G, seeds, hop_limit=hop_lim, per_seed_cap=per_cap, global_cap=glob_cap)\n",
    "\n",
    "    # prioritize seeds, then expanded unique\n",
    "    cand = []\n",
    "    seen = set()\n",
    "    for c in seeds + expanded:\n",
    "        if c not in seen:\n",
    "            cand.append(c); seen.add(c)\n",
    "\n",
    "    # MMR selection for context\n",
    "    try:\n",
    "        selected = mmr_select(embedder, qv, cand, chunks, k=mmr_k, lambda_weight=mmr_lmb)\n",
    "    except Exception:\n",
    "        # fallback: take first mmr_k\n",
    "        selected = cand[:mmr_k]\n",
    "\n",
    "    context = build_context(meta, chunks, selected, max_chars=max_chars)\n",
    "\n",
    "    with colL:\n",
    "        st.markdown(\"### Top Sources (MMR-selected)\")\n",
    "        for i, cid in enumerate(selected, 1):\n",
    "            row = meta.loc[meta[\"chunk_id\"]==cid].iloc[0]\n",
    "            st.write(f\"[{i}] {row['path']} (chunk {row['chunk_idx']})\")\n",
    "\n",
    "        st.markdown(\"### Context Preview\")\n",
    "        st.code(context[:1000] + (\" ...\" if len(context) > 1000 else \"\"), language=\"text\")\n",
    "\n",
    "    answer = \"\"\n",
    "    lat = None\n",
    "    with colR:\n",
    "        if load_llm:\n",
    "            try:\n",
    "                t1 = time.time()\n",
    "                answer = qwen.answer(query, context, max_new_tokens=256)\n",
    "                lat = round(time.time() - t1, 2)\n",
    "                st.markdown(\"### Answer\")\n",
    "                st.write(answer)\n",
    "                st.caption(f\"LLM latency: {lat}s\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Answer error: {e}\")\n",
    "        else:\n",
    "            st.info(\"Load the Qwen model in the sidebar to generate an answer.\")\n",
    "            st.markdown(\"### Answer (not generated)\")\n",
    "            st.write(\"\")\n",
    "\n",
    "    # Log row\n",
    "    try:\n",
    "        LOG_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "        file_exists = LOG_CSV.exists()\n",
    "        with open(LOG_CSV, \"a\", newline=\"\") as f:\n",
    "            w = csv.DictWriter(f, fieldnames=[\n",
    "                \"ts\",\"query\",\"top_k\",\"hop_limit\",\"per_seed_cap\",\"global_cap\",\"mmr_k\",\"mmr_lambda\",\"latency_s\",\"seeds\",\"expanded_size\",\"selected_ids\"\n",
    "            ])\n",
    "            if not file_exists:\n",
    "                w.writeheader()\n",
    "            w.writerow({\n",
    "                \"ts\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"query\": query,\n",
    "                \"top_k\": top_k,\n",
    "                \"hop_limit\": hop_lim,\n",
    "                \"per_seed_cap\": per_cap,\n",
    "                \"global_cap\": glob_cap,\n",
    "                \"mmr_k\": mmr_k,\n",
    "                \"mmr_lambda\": mmr_lmb,\n",
    "                \"latency_s\": lat if lat is not None else \"\",\n",
    "                \"seeds\": str(seeds),\n",
    "                \"expanded_size\": len(expanded),\n",
    "                \"selected_ids\": str(selected)\n",
    "            })\n",
    "    except Exception as e:\n",
    "        st.warning(f\"Could not write log: {e}\")\n",
    "\n",
    "    st.success(f\"Done in {round(time.time()-t0, 2)}s.\")\n",
    "'''\n",
    "\n",
    "APP_PATH.write_text(app_code)\n",
    "print(f\"Wrote app to: {APP_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a599ca8b-20f3-4d09-b86c-899d0256312e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Deprecated cell — see 'Execution Notes' above. Use Cell K + terminal launch instead.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m Deprecated cell — see 'Execution Notes' above. Use Cell K + terminal launch instead.\n"
     ]
    }
   ],
   "source": [
    "# Deprecated / Skip\n",
    "raise SystemExit(\"Deprecated cell — see 'Execution Notes' above. Use Cell K + terminal launch instead.\")\n",
    "\n",
    "\"\"\"\n",
    "'Cell L: Run Streamlit'\n",
    "- Launch the app in this environment.\n",
    "- Stop with Ctrl+C in terminal if launched from a terminal; via notebook, it'll stream logs until you interrupt.\n",
    "\"\"\"\n",
    "import os, subprocess, sys\n",
    "subprocess.run([\"bash\",\"-lc\", f\"cd /home/manny-buff/projects/capstone/week6-rag-graph/app && streamlit run app_rag.py\"], check=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff1a99-93fd-46ad-b3ed-c09a2e2cf64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week 6 — Part 3: Application (Summary)\n",
    "'''\n",
    "**UI**: Streamlit single-file app (`app/app_rag.py`).  \n",
    "**Pipeline**: Dense retrieval → Graph expansion → MMR re-ranking → Context build → Qwen (deterministic).  \n",
    "**Model**: Qwen-VL (local path preferred), loaded once and cached in `st.session_state`.  \n",
    "**Determinism**: `do_sample=False`; no temperature/top-p/top-k.  \n",
    "**Logs**: `artifacts/app_logs.csv` (query, settings, seeds, expanded size, selected IDs, latency).  \n",
    "\n",
    "### How to run\n",
    "1. Write app from Cell K (once).  \n",
    "2. Terminal:  \n",
    "    source ~/venvs/core-rag/bin/activate\n",
    "    STREAMLIT_BROWSER_GATHER_USAGE_STATS=false\n",
    "    streamlit run /home/manny-buff/projects/capstone/week6-rag-graph/app/app_rag.py\n",
    "        --server.headless true --server.port 8501\n",
    "3. In the UI, check **Load Qwen model**, ask questions.\n",
    "\n",
    "### Validations observed\n",
    "- Top sources reflect relevant chunks from the FAISS+Graph pipeline.\n",
    "- Context preview trimmed to the configured character budget.\n",
    "- Answers generated without prompt echo or HF noise.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c415d69-3dbd-4c84-896e-a9554795d384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752254b0aa6f43a9a295309e20c89a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /home/manny-buff/projects/capstone/week6-rag-graph/artifacts/Report_snippets_Wk6_3.md\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Report Retrieval Script #2 — CLEAN VERSION (Week6-3)'\n",
    "- Deterministic beam search with anti-repetition.\n",
    "- Bullet de-duplication for cleaner report text.\n",
    "- Outputs: artifacts/Report_snippets_Wk6_3.md\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import json, pickle, time, textwrap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph\")\n",
    "CFG_RUN = ROOT / \"configs\" / \"rag_graph_run_config.json\"\n",
    "VDB     = ROOT / \"artifacts\" / \"vdb\"\n",
    "ART_META   = VDB / \"chunks_meta.parquet\"\n",
    "ART_CHUNKS = VDB / \"chunks_text.pkl\"\n",
    "ART_FAISS  = VDB / \"faiss.index\"\n",
    "ART_GRAPH  = VDB / \"graph.pkl\"\n",
    "LOG_CSV    = ROOT / \"artifacts\" / \"app_logs.csv\"\n",
    "OUT_MD     = ROOT / \"artifacts\" / \"Report_snippets_Wk6_3.md\"\n",
    "\n",
    "# ---- Load config & artifacts\n",
    "cfg = json.loads(CFG_RUN.read_text())\n",
    "meta   = pd.read_parquet(ART_META)\n",
    "chunks = pickle.loads(ART_CHUNKS.read_bytes())\n",
    "\n",
    "import faiss, networkx as nx\n",
    "index  = faiss.read_index(str(ART_FAISS))\n",
    "with open(ART_GRAPH, \"rb\") as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer(cfg[\"embed_model\"])\n",
    "\n",
    "# ---- Qwen loader (VL-aware)\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig, AutoProcessor, AutoModelForImageTextToText,\n",
    "    AutoTokenizer, AutoModelForCausalLM\n",
    ")\n",
    "from transformers.utils import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "GEN_KW = dict(                 # anti-repetition, deterministic\n",
    "    do_sample=False,\n",
    "    num_beams=4,\n",
    "    early_stopping=True,\n",
    "    no_repeat_ngram_size=6,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "class QwenAnswerer:\n",
    "    def __init__(self, local_dir: Path, model_id: str):\n",
    "        self.local_dir = Path(local_dir)\n",
    "        self.model_id  = model_id\n",
    "        self.is_vl     = False\n",
    "        self.processor = None\n",
    "        self.tokenizer = None\n",
    "        self.model     = None\n",
    "    def _load_from(self, src: str):\n",
    "        cfg = AutoConfig.from_pretrained(src, trust_remote_code=True)\n",
    "        mtype = getattr(cfg, \"model_type\", \"\").lower()\n",
    "        if \"vl\" in mtype:\n",
    "            self.is_vl = True\n",
    "            self.processor = AutoProcessor.from_pretrained(src, trust_remote_code=True, use_fast=False)\n",
    "            self.model = AutoModelForImageTextToText.from_pretrained(\n",
    "                src, trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "        else:\n",
    "            self.is_vl = False\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(src, trust_remote_code=True)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                src, trust_remote_code=True,\n",
    "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "    def load(self):\n",
    "        last = None\n",
    "        for src in ([str(self.local_dir)] if self.local_dir.exists() else []) + [self.model_id]:\n",
    "            try:\n",
    "                self._load_from(src); return self\n",
    "            except Exception as e: last = e\n",
    "        raise RuntimeError(f\"Qwen load failed: {last}\")\n",
    "\n",
    "    def _prompt(self, q, ctx):\n",
    "        sys = \"You are a concise home-repair RAG assistant. Use ONLY the provided context.\"\n",
    "        user = (\n",
    "            f\"Question:\\n{q}\\n\\nContext:\\n{ctx}\\n\\n\"\n",
    "            \"Respond in 4–8 bullet points, each UNIQUE and action-oriented. \"\n",
    "            \"End with a bracketed list of 2–4 source file names.\\n\"\n",
    "        )\n",
    "        tok = self.processor.tokenizer if self.is_vl else self.tokenizer\n",
    "        apply_chat = getattr(tok, \"apply_chat_template\", None)\n",
    "        if callable(apply_chat):\n",
    "            msgs = [{\"role\":\"system\",\"content\":sys},{\"role\":\"user\",\"content\":user}]\n",
    "            return apply_chat(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        return f\"{sys}\\n\\n{user}\"\n",
    "\n",
    "    def _dedupe_bullets(self, text: str, min_b=4, max_b=8) -> str:\n",
    "        lines = [ln.strip() for ln in text.splitlines()]\n",
    "        seen, out = set(), []\n",
    "        for ln in lines:\n",
    "            if not ln: continue\n",
    "            if ln.startswith((\"-\", \"*\", \"•\")):\n",
    "                core = ln.lstrip(\"-*• \").strip()\n",
    "                key  = core.lower()\n",
    "                if key in seen: \n",
    "                    continue\n",
    "                seen.add(key)\n",
    "                out.append(\"- \" + core)\n",
    "            else:\n",
    "                out.append(ln)\n",
    "        # Clamp bullet count\n",
    "        bullets = [l for l in out if l.startswith(\"- \")]\n",
    "        if len(bullets) > max_b:\n",
    "            kept, rest = 0, []\n",
    "            new_out = []\n",
    "            for l in out:\n",
    "                if l.startswith(\"- \") and kept < max_b:\n",
    "                    new_out.append(l); kept += 1\n",
    "                elif not l.startswith(\"- \"):\n",
    "                    rest.append(l)\n",
    "            out = new_out + rest\n",
    "        return \"\\n\".join(out).strip()\n",
    "\n",
    "    def answer(self, q, ctx, max_new_tokens=220):\n",
    "        ptxt = self._prompt(q, ctx)\n",
    "        if self.is_vl:\n",
    "            inp = self.processor(text=ptxt, return_tensors=\"pt\")\n",
    "            inp = {k:v.to(self.model.device) for k,v in inp.items()}\n",
    "            inlen = int(inp[\"input_ids\"].shape[1])\n",
    "            with torch.no_grad():\n",
    "                out = self.model.generate(**inp, max_new_tokens=max_new_tokens, **GEN_KW)\n",
    "            gen = out[0][inlen:]\n",
    "            txt = self.processor.tokenizer.decode(gen, skip_special_tokens=True)\n",
    "        else:\n",
    "            inp = self.tokenizer(ptxt, return_tensors=\"pt\").to(self.model.device)\n",
    "            inlen = int(inp[\"input_ids\"].shape[1])\n",
    "            with torch.no_grad():\n",
    "                out = self.model.generate(**inp, max_new_tokens=max_new_tokens, **GEN_KW)\n",
    "            gen = out[0][inlen:]\n",
    "            txt = self.tokenizer.decode(gen, skip_special_tokens=True)\n",
    "        return self._dedupe_bullets(txt)\n",
    "\n",
    "qwen = QwenAnswerer(cfg[\"llm_local_path\"], cfg[\"llm_model_id\"]).load()\n",
    "\n",
    "# ---- Retrieval helpers\n",
    "def dense_retrieve(query: str, top_k: int):\n",
    "    qv = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    D, I = index.search(qv.astype(np.float32), top_k)\n",
    "    return qv[0], [int(x) for x in I[0]]\n",
    "\n",
    "def expand_via_graph(seeds, hop_limit: int, per_seed_cap: int = 20, global_cap: int = 200):\n",
    "    visited = set(int(s) for s in seeds); frontier = set(visited)\n",
    "    for _ in range(hop_limit):\n",
    "        nxt = set()\n",
    "        for node in list(frontier):\n",
    "            for nb in list(G.neighbors(node))[:per_seed_cap]:\n",
    "                nb = int(nb)\n",
    "                if nb not in visited: nxt.add(nb)\n",
    "        frontier = nxt; visited.update(frontier)\n",
    "        if len(visited) >= global_cap: break\n",
    "    return list(visited)\n",
    "\n",
    "def mmr_select(query_vec, cand_ids, k=12, lambda_weight=0.70):\n",
    "    if not cand_ids: return []\n",
    "    C = embedder.encode([chunks[int(c)] for c in cand_ids],\n",
    "                        convert_to_numpy=True, normalize_embeddings=True)\n",
    "    rel = C @ query_vec\n",
    "    selected, pool = [], list(range(len(cand_ids)))\n",
    "    while pool and len(selected) < k:\n",
    "        if not selected:\n",
    "            best = int(np.argmax(rel[pool])); chosen = pool.pop(best); selected.append(chosen); continue\n",
    "        sim_to_S = C[pool] @ C[selected].T\n",
    "        max_div = sim_to_S.max(axis=1) if sim_to_S.ndim == 2 else sim_to_S\n",
    "        mmr = lambda_weight * rel[pool] - (1.0 - lambda_weight) * max_div\n",
    "        best = int(np.argmax(mmr)); chosen = pool.pop(best); selected.append(chosen)\n",
    "    return [int(cand_ids[i]) for i in selected]\n",
    "\n",
    "def build_context(ids, max_chars=4000):\n",
    "    if not ids: return \"\"\n",
    "    out, size = [], 0\n",
    "    for cid in ids:\n",
    "        row = meta.loc[meta[\"chunk_id\"]==int(cid)]\n",
    "        if row.empty: continue\n",
    "        path = row.iloc[0][\"path\"]; idx = int(row.iloc[0][\"chunk_idx\"])\n",
    "        snip = textwrap.shorten(chunks[int(cid)], width=360, placeholder=\" …\")\n",
    "        block = f\"[SOURCE] {path} | chunk#{idx}\\n{snip}\\n\"\n",
    "        if size + len(block) > max_chars: break\n",
    "        out.append(block); size += len(block)\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "# ---- Build snippet\n",
    "lines = [\"# Week 6 — Report Snippets (Part 3: Application)\\n\"]\n",
    "if not LOG_CSV.exists():\n",
    "    lines.append(\"_No app logs yet (artifacts/app_logs.csv missing)._\")\n",
    "    OUT_MD.write_text(\"\\n\".join(lines)); print(f\"Wrote: {OUT_MD}\"); raise SystemExit\n",
    "\n",
    "logs = pd.read_csv(LOG_CSV)\n",
    "runs = len(logs); lat = logs[\"latency_s\"].dropna()\n",
    "avg_lat = lat.mean() if not lat.empty else None\n",
    "lines += [f\"- Total runs: **{runs}**\",\n",
    "          f\"- Average LLM latency: **{avg_lat:.2f}s**\" if avg_lat is not None else \"- Average LLM latency: _n/a_\",\n",
    "          \"\"]\n",
    "\n",
    "N = min(3, runs)\n",
    "for i, row in logs.tail(N).reset_index(drop=True).iterrows():\n",
    "    q = str(row[\"query\"])\n",
    "    top_k    = int(row.get(\"top_k\", cfg.get(\"retriever_k\", 5)))\n",
    "    hop_lim  = int(row.get(\"hop_limit\", cfg.get(\"hop_limit\", 2)))\n",
    "    per_cap  = int(row.get(\"per_seed_cap\", 20))\n",
    "    glob_cap = int(row.get(\"global_cap\", 200))\n",
    "    mmr_k    = int(row.get(\"mmr_k\", 12))\n",
    "    mmr_lmb  = float(row.get(\"mmr_lambda\", 0.70))\n",
    "\n",
    "    qv, seeds = dense_retrieve(q, top_k)\n",
    "    expanded  = expand_via_graph(seeds, hop_lim, per_seed_cap=per_cap, global_cap=glob_cap)\n",
    "    cand, seen = [], set()\n",
    "    for c in seeds + expanded:\n",
    "        c = int(c)\n",
    "        if c not in seen: cand.append(c); seen.add(c)\n",
    "    selected = mmr_select(qv, cand, k=mmr_k, lambda_weight=mmr_lmb) or cand[:mmr_k]\n",
    "    ctx = build_context(selected, max_chars=4000)\n",
    "\n",
    "    t0 = time.time()\n",
    "    ans = qwen.answer(q, ctx, max_new_tokens=240)\n",
    "    lat2 = round(time.time() - t0, 2)\n",
    "\n",
    "    srcs = []\n",
    "    for cid in selected[:6]:\n",
    "        rm = meta.loc[meta[\"chunk_id\"]==int(cid)]\n",
    "        if rm.empty: continue\n",
    "        srcs.append(Path(rm.iloc[0][\"path\"]).name)\n",
    "    srcs_txt = \"; \".join(dict.fromkeys(srcs))\n",
    "\n",
    "    lines += [\n",
    "        f\"## Sample {i+1}\",\n",
    "        f\"**Query:** {q}\",\n",
    "        f\"**Settings:** K={top_k}, hops={hop_lim}, per_cap={per_cap}, global_cap={glob_cap}, MMR_k={mmr_k}, λ={mmr_lmb}\",\n",
    "        f\"**Answer (lat {lat2}s):**\\n\\n{ans}\\n\",\n",
    "        f\"**Sources:** {srcs_txt}\\n\"\n",
    "    ]\n",
    "\n",
    "OUT_MD.write_text(\"\\n\".join(lines))\n",
    "print(f\"Wrote: {OUT_MD}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc9e01e2-864d-4084-8c92-40ede6ac76fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /home/manny-buff/projects/capstone/week6-rag-graph/Report.md\n",
      "\n",
      "Report preview:\n",
      "['# Capstone — Week 6: Next-Level RAG (Graph-RAG → Multi-Hop → Application)', '', '**Author:** manny-buff  ', '**Generated:** 2025-10-05 16:28:47', '', 'This report compiles:', '- Part 1: Graph-RAG Build', '- Part 2: Multi-Hop QA (dense → graph expansion → Qwen synthesis)', '- Part 3: Streamlit Application (logs & sample answers)', '', 'Artifacts are created under `artifacts/` per the run config in `configs/rag_graph_run_config.json`.', '', '---', '', '# Week 6 — Report Snippets (Parts 1 & 2)', '', '## Environment', '- Python venv: `/home/manny-buff/venvs/core-rag`', '- Python: `3.11.9`', '- GPU: `NVIDIA GeForce RTX 4080, 580.65.06, 16376 MiB`', '', '## Run Config', '- corpus_root: `/home/manny-buff/projects/capstone/hw-rag/data/`', '- embed_model: `intfloat/e5-small-v2`', '- llm_model_id: `Qwen/Qwen2.5-VL-3B-Instruct`', '- llm_local_path: `/home/manny-buff/projects/capstone/hw-rag/models/Qwen2-VL-2B-Instruct/`', '- device: `cuda`', '- retriever_k: `5`', '- hop_limit: `2`', '']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell R: Assemble Report.md'\n",
    "Purpose:\n",
    "- Combine Week6 Part 1&2 snippet + Part 3 snippet into a single Report.md at the project root.\n",
    "- Adds a short cover with project info and run timestamp.\n",
    "\n",
    "Outputs:\n",
    "- /home/manny-buff/projects/capstone/week6-rag-graph/Report.md\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "ROOT = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph\")\n",
    "SNIP_12 = ROOT / \"artifacts\" / \"Report_snippets_Wk6_1_2.md\"\n",
    "SNIP_3  = ROOT / \"artifacts\" / \"Report_snippets_Wk6_3.md\"\n",
    "OUT     = ROOT / \"Report.md\"\n",
    "\n",
    "# 'Read snippets safely (empty if missing)'\n",
    "snip12 = SNIP_12.read_text() if SNIP_12.exists() else \"_Part 1&2 snippet missing._\"\n",
    "snip3  = SNIP_3.read_text()  if SNIP_3.exists()  else \"_Part 3 snippet missing._\"\n",
    "\n",
    "# 'Cover'\n",
    "cover = f\"\"\"# Capstone — Week 6: Next-Level RAG (Graph-RAG → Multi-Hop → Application)\n",
    "\n",
    "**Author:** manny-buff  \n",
    "**Generated:** {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "This report compiles:\n",
    "- Part 1: Graph-RAG Build\n",
    "- Part 2: Multi-Hop QA (dense → graph expansion → Qwen synthesis)\n",
    "- Part 3: Streamlit Application (logs & sample answers)\n",
    "\n",
    "Artifacts are created under `artifacts/` per the run config in `configs/rag_graph_run_config.json`.\n",
    "\n",
    "---\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 'Assemble and write'\n",
    "OUT.write_text(cover + snip12 + \"\\n\\n---\\n\\n\" + snip3 + \"\\n\")\n",
    "print(f\"Wrote: {OUT}\")\n",
    "print(\"\\nReport preview:\")\n",
    "print(OUT.read_text().splitlines()[0:30])  # show first ~30 lines as a quick probe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b29c7b04-a40a-4f28-86fe-29fed40d4938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote:\n",
      "- /home/manny-buff/projects/capstone/week6-rag-graph/.gitignore (size=444 bytes)\n",
      "\n",
      "README preview:\n",
      "['# Qwen Graph-RAG — Week 6', '', 'This repo contains a local, config-driven Graph-RAG pipeline with Multi-Hop QA and a Streamlit application.', 'It reuses the Week5 dataset and Qwen model paths, and writes reproducible configs and artifacts.', '', '## Quickstart', '', '', '# 1) Python env', 'python -m venv ~/venvs/core-rag', 'source ~/venvs/core-rag/bin/activate', 'pip install -r requirements.txt', '', '# 2) Build index + graph (Week6-1)', '# open notebooks/Week6-1-HW.ipynb and run Cells C→F', '', '# 3) Multi-Hop QA (Week6-2)', '# open notebooks/Week6-2-HW.ipynb and run Cells G→J', '# (ensure you see ablation rows in artifacts/ablation_results_graph.csv)', '', '# 4) Application (Week6-3)', '# run Cell K to write the app file (once), then from terminal:', 'streamlit run app/app_rag.py --server.headless true --server.port 8501', '']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Cell S: Generate README.md, requirements.txt, and .gitignore'\n",
    "Purpose:\n",
    "- Create concise, reproducible project metadata files at the repo root.\n",
    "\n",
    "Files:\n",
    "- README.md\n",
    "- requirements.txt\n",
    "- .gitignore\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "ROOT = Path(\"/home/manny-buff/projects/capstone/week6-rag-graph\")\n",
    "\n",
    "readme = f\"\"\"# Qwen Graph-RAG — Week 6\n",
    "\n",
    "This repo contains a local, config-driven Graph-RAG pipeline with Multi-Hop QA and a Streamlit application.\n",
    "It reuses the Week5 dataset and Qwen model paths, and writes reproducible configs and artifacts.\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "\n",
    "# 1) Python env\n",
    "python -m venv ~/venvs/core-rag\n",
    "source ~/venvs/core-rag/bin/activate\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 2) Build index + graph (Week6-1)\n",
    "# open notebooks/Week6-1-HW.ipynb and run Cells C→F\n",
    "\n",
    "# 3) Multi-Hop QA (Week6-2)\n",
    "# open notebooks/Week6-2-HW.ipynb and run Cells G→J\n",
    "# (ensure you see ablation rows in artifacts/ablation_results_graph.csv)\n",
    "\n",
    "# 4) Application (Week6-3)\n",
    "# run Cell K to write the app file (once), then from terminal:\n",
    "streamlit run app/app_rag.py --server.headless true --server.port 8501\n",
    "\n",
    "Structure\n",
    "week6-rag-graph/\n",
    "├─ app/\n",
    "│  └─ app_rag.py\n",
    "├─ artifacts/\n",
    "│  ├─ vdb/                # FAISS index, graph, chunk meta\n",
    "│  ├─ app_logs.csv        # Streamlit runs (app appends)\n",
    "│  ├─ ablation_results_graph.csv\n",
    "│  ├─ Report_snippets_Wk6_1_2.md\n",
    "│  └─ Report_snippets_Wk6_3.md\n",
    "├─ configs/\n",
    "│  ├─ env_rag_graph.json\n",
    "│  └─ rag_graph_run_config.json\n",
    "├─ notebooks/\n",
    "│  ├─ Week6-1-HW.ipynb    # Build\n",
    "│  ├─ Week6-2-HW.ipynb    # Multi-Hop QA\n",
    "│  └─ Week6-3-HW.ipynb    # Application + reporting\n",
    "└─ Report.md\n",
    "\n",
    "Configuration\n",
    "\n",
    "Update configs/rag_graph_run_config.json to point at:\n",
    "\n",
    "\"corpus_root\": \"/home/manny-buff/projects/capstone/hw-rag/data/\"\n",
    "\n",
    "\"llm_local_path\": \"/home/manny-buff/projects/capstone/hw-rag/models/Qwen2-VL-2B-Instruct/\"\n",
    "\n",
    "\"embed_model\": \"intfloat/e5-small-v2\"\n",
    "\n",
    "\"llm_model_id\": \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "Notes\n",
    "\n",
    "Deterministic generation (do_sample=False) for evaluation.\n",
    "\n",
    "Beam search with no-repeat for report regeneration to reduce repetition.\n",
    "\"\"\"\n",
    "\n",
    "requirements = \"\"\"# Core\n",
    "numpy\n",
    "pandas\n",
    "networkx\n",
    "matplotlib\n",
    "\n",
    "Embeddings / LLM\n",
    "\n",
    "sentence-transformers\n",
    "transformers>=4.45.0\n",
    "accelerate>=0.34.0\n",
    "safetensors\n",
    "\n",
    "Vector store / parsing\n",
    "\n",
    "faiss-cpu\n",
    "pypdf\n",
    "beautifulsoup4\n",
    "lxml\n",
    "tqdm\n",
    "\n",
    "App\n",
    "\n",
    "streamlit\n",
    "\"\"\"\n",
    "\n",
    "gitignore = \"\"\"# Python\n",
    "pycache/\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.pyd\n",
    "*.egg-info/\n",
    ".ipynb_checkpoints/\n",
    ".DS_Store\n",
    "\n",
    "Virtual envs (keep them local)\n",
    "\n",
    ".venv/\n",
    "venv/\n",
    "env/\n",
    "venvs/\n",
    "*.env\n",
    "\n",
    "Large, derived, or local-only artifacts\n",
    "\n",
    "artifacts/vdb/*\n",
    "artifacts/app_logs.csv\n",
    "\n",
    "Keep important small outputs\n",
    "\n",
    "!artifacts/ablation_results_graph.csv\n",
    "!artifacts/Report_snippets_Wk6_1_2.md\n",
    "!artifacts/Report_snippets_Wk6_3.md\n",
    "\n",
    "Models / data paths are external (do not commit local copies)\n",
    "\n",
    "models/\n",
    "data/\n",
    "\"\"\"\n",
    "\n",
    "# Write files\n",
    "\n",
    "(ROOT / \"README.md\").write_text(readme)\n",
    "(ROOT / \"requirements.txt\").write_text(requirements)\n",
    "(ROOT / \".gitignore\").write_text(gitignore)\n",
    "\n",
    "print(\"Wrote:\")\n",
    "for f in [\"README.md\", \"requirements.txt\", \".gitignore\"]:\n",
    "    p = ROOT / f\n",
    "print(\"-\", p, f\"(size={p.stat().st_size} bytes)\")\n",
    "\n",
    "print(\"\\nREADME preview:\")\n",
    "print((ROOT/\"README.md\").read_text().splitlines()[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e9360-8a73-40ec-9347-4791937dc42a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (core-rag)",
   "language": "python",
   "name": "core-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
